{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (17,), float64)\n",
      "Action space: Box(-1.0, 1.0, (6,), float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('HalfCheetah-v4')\n",
    "\n",
    "print(f'Observation space: {env.observation_space}')\n",
    "print(f'Action space: {env.action_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "lr = 0.0005\n",
    "episodes = 2000\n",
    "hid_layer = 512\n",
    "hid_layer2 = 512\n",
    "randomness_begin = 1.0\n",
    "randomness_end = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class A2C(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim2, action_dim):\n",
    "        super(A2C, self).__init__()\n",
    "        self.common = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim2),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.actor_mu = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim2, action_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        self.actor_var = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim2, action_dim),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "        self.critic = torch.nn.Linear(hidden_dim2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        common_out = self.common(x)\n",
    "        return self.actor_mu(common_out), self.actor_var(common_out), self.critic(common_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class AgentA2C():\n",
    "    def __init__(self, model, optim, device, eps_start, eps_end, eps_decay_time, loss, entropy_beta):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.device = device\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay_time = eps_decay_time\n",
    "        self.steps_counter = 0\n",
    "        self.episodes_counter = 0\n",
    "        self.loss = loss\n",
    "        self.entropy_beta = entropy_beta\n",
    "\n",
    "\n",
    "    def calc_logprob(self, mu, var, action):\n",
    "        p1 = - ((mu - action) ** 2) / (2*var.clamp(min=1e-3))\n",
    "        p2 = - torch.log(torch.sqrt(2* math.pi* var))\n",
    "        return p1 + p2\n",
    "\n",
    "    def run_episode(self):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        state = torch.tensor(env.reset()[0], dtype=torch.float).to(self.device)\n",
    "\n",
    "        total_loss = 0.\n",
    "        total_actor_loss = 0.\n",
    "        total_critic_loss = 0.\n",
    "        steps = 0\n",
    "        risks = 0\n",
    "\n",
    "        while not done:\n",
    "            mu, var, value = self.model(state)\n",
    "\n",
    "            std = torch.sqrt(var).data.cpu()\n",
    "            log_std = torch.log(std).to(self.device)\n",
    "            std = std.numpy()\n",
    "\n",
    "            eps = self.eps_end + (self.eps_start - self.eps_end) * (1 - min(self.episodes_counter, self.eps_decay_time)/self.eps_decay_time)\n",
    "\n",
    "            risk = random.random() <= eps\n",
    "\n",
    "            if risk:\n",
    "                risks += 1\n",
    "\n",
    "            action = np.random.normal(mu, std) if not risk else np.random.uniform(-1, 1, 6)\n",
    "\n",
    "            state, r, done, _, _ = env.step(action)\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            _, _, new_value = self.model(state)\n",
    "\n",
    "            TD_err = r + gamma*new_value*(1 - int(done)) - value\n",
    "\n",
    "            critic_loss = (TD_err**2).mean()\n",
    "\n",
    "            action = torch.tensor(action, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            log_prob = self.calc_logprob(mu, var, action)\n",
    "\n",
    "            actor_loss = (-log_prob * TD_err).mean()\n",
    "\n",
    "            entropy_loss = (self.entropy_beta * (-(torch.log(2*math.pi*var) + 1)/2)).mean()\n",
    "\n",
    "            loss = critic_loss+actor_loss+entropy_loss\n",
    "\n",
    "            total_loss += loss\n",
    "            total_actor_loss += actor_loss\n",
    "            total_critic_loss += critic_loss\n",
    "\n",
    "            print(f'Actor loss: {actor_loss}')\n",
    "            print(f'Critic loss: {critic_loss}')\n",
    "            print(f'Entropy loss: {entropy_loss}')\n",
    "            print(f'Loss: {loss}')\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            self.optim.step()\n",
    "            self.steps_counter += 1\n",
    "\n",
    "        self.episodes_counter += 1\n",
    "\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(env.observation_space.shape[0], hid_layer, hid_layer2, env.action_space.shape[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epsilon_decay_time = 1000\n",
    "entropy_beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentA2C(model, optimizer, device, randomness_begin, randomness_end, epsilon_decay_time, loss_fn, entropy_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss: -0.17245137691497803\n",
      "Critic loss: 0.026317566633224487\n",
      "Entropy loss: -0.01236751675605774\n",
      "Loss: -0.15850132703781128\n",
      "Actor loss: 1.1307529211044312\n",
      "Critic loss: 1.088650107383728\n",
      "Entropy loss: -0.012605908326804638\n",
      "Loss: 2.2067971229553223\n",
      "Actor loss: 0.47797632217407227\n",
      "Critic loss: 0.2523984909057617\n",
      "Entropy loss: -0.012242275290191174\n",
      "Loss: 0.7181325554847717\n",
      "Actor loss: -0.10669003427028656\n",
      "Critic loss: 0.01062803715467453\n",
      "Entropy loss: -0.012842963449656963\n",
      "Loss: -0.10890495777130127\n",
      "Actor loss: 0.20158110558986664\n",
      "Critic loss: 0.04163181781768799\n",
      "Entropy loss: -0.012707000598311424\n",
      "Loss: 0.23050592839717865\n",
      "Actor loss: 0.811187744140625\n",
      "Critic loss: 0.8126053214073181\n",
      "Entropy loss: -0.012248784303665161\n",
      "Loss: 1.6115443706512451\n",
      "Actor loss: 0.17117080092430115\n",
      "Critic loss: 0.030517254024744034\n",
      "Entropy loss: -0.011908305808901787\n",
      "Loss: 0.18977974355220795\n",
      "Actor loss: -0.031254447996616364\n",
      "Critic loss: 0.0009941385360434651\n",
      "Entropy loss: -0.012503686361014843\n",
      "Loss: -0.04276399686932564\n",
      "Actor loss: -2.6137304306030273\n",
      "Critic loss: 4.175470352172852\n",
      "Entropy loss: -0.011787409894168377\n",
      "Loss: 1.549952507019043\n",
      "Actor loss: -0.5489710569381714\n",
      "Critic loss: 0.37325769662857056\n",
      "Entropy loss: -0.011412288062274456\n",
      "Loss: -0.18712565302848816\n",
      "Actor loss: -1.8365925550460815\n",
      "Critic loss: 2.184593439102173\n",
      "Entropy loss: -0.011698096059262753\n",
      "Loss: 0.336302787065506\n",
      "Actor loss: -0.11806299537420273\n",
      "Critic loss: 0.016775716096162796\n",
      "Entropy loss: -0.012129165232181549\n",
      "Loss: -0.11341644078493118\n",
      "Actor loss: -0.6272885799407959\n",
      "Critic loss: 0.3308895528316498\n",
      "Entropy loss: -0.0123521089553833\n",
      "Loss: -0.3087511360645294\n",
      "Actor loss: -0.5295792818069458\n",
      "Critic loss: 0.3070746064186096\n",
      "Entropy loss: -0.011894458904862404\n",
      "Loss: -0.23439913988113403\n",
      "Actor loss: -2.498898983001709\n",
      "Critic loss: 3.4307563304901123\n",
      "Entropy loss: -0.01206827163696289\n",
      "Loss: 0.9197890758514404\n",
      "Actor loss: 1.0411596298217773\n",
      "Critic loss: 0.46825841069221497\n",
      "Entropy loss: -0.01223842054605484\n",
      "Loss: 1.497179627418518\n",
      "Actor loss: 1.0458652973175049\n",
      "Critic loss: 0.2722543776035309\n",
      "Entropy loss: -0.011350573971867561\n",
      "Loss: 1.3067690134048462\n",
      "Actor loss: 0.843233585357666\n",
      "Critic loss: 0.4229428470134735\n",
      "Entropy loss: -0.012211262248456478\n",
      "Loss: 1.2539652585983276\n",
      "Actor loss: -2.5348784923553467\n",
      "Critic loss: 1.8693428039550781\n",
      "Entropy loss: -0.01157824695110321\n",
      "Loss: -0.677113950252533\n",
      "Actor loss: 1.6407246589660645\n",
      "Critic loss: 1.4783929586410522\n",
      "Entropy loss: -0.012221707962453365\n",
      "Loss: 3.106895923614502\n",
      "Actor loss: 1.917684555053711\n",
      "Critic loss: 1.9068423509597778\n",
      "Entropy loss: -0.012208416126668453\n",
      "Loss: 3.8123183250427246\n",
      "Actor loss: -2.0759687423706055\n",
      "Critic loss: 3.9871182441711426\n",
      "Entropy loss: -0.012255321256816387\n",
      "Loss: 1.898894190788269\n",
      "Actor loss: -0.8466806411743164\n",
      "Critic loss: 0.22693562507629395\n",
      "Entropy loss: -0.012480209581553936\n",
      "Loss: -0.6322252154350281\n",
      "Actor loss: -0.43742674589157104\n",
      "Critic loss: 0.11277520656585693\n",
      "Entropy loss: -0.012181851081550121\n",
      "Loss: -0.3368333876132965\n",
      "Actor loss: 3.4274539947509766\n",
      "Critic loss: 3.9794974327087402\n",
      "Entropy loss: -0.012044323608279228\n",
      "Loss: 7.394906997680664\n",
      "Actor loss: -0.10531439632177353\n",
      "Critic loss: 0.019491391256451607\n",
      "Entropy loss: -0.011692896485328674\n",
      "Loss: -0.09751590341329575\n",
      "Actor loss: -1.943436861038208\n",
      "Critic loss: 2.1005873680114746\n",
      "Entropy loss: -0.012235572561621666\n",
      "Loss: 0.14491493999958038\n",
      "Actor loss: 1.287909984588623\n",
      "Critic loss: 0.49767568707466125\n",
      "Entropy loss: -0.011969238519668579\n",
      "Loss: 1.7736164331436157\n",
      "Actor loss: 0.6977261900901794\n",
      "Critic loss: 0.1871132254600525\n",
      "Entropy loss: -0.011910809203982353\n",
      "Loss: 0.8729286193847656\n",
      "Actor loss: -1.0317208766937256\n",
      "Critic loss: 0.9115323424339294\n",
      "Entropy loss: -0.012098543345928192\n",
      "Loss: -0.13228708505630493\n",
      "Actor loss: 2.161058187484741\n",
      "Critic loss: 0.9283000230789185\n",
      "Entropy loss: -0.012103846296668053\n",
      "Loss: 3.0772545337677\n",
      "Actor loss: -4.9721808433532715\n",
      "Critic loss: 18.933204650878906\n",
      "Entropy loss: -0.012687092646956444\n",
      "Loss: 13.948336601257324\n",
      "Actor loss: 0.7351657748222351\n",
      "Critic loss: 0.15672332048416138\n",
      "Entropy loss: -0.01110741775482893\n",
      "Loss: 0.8807816505432129\n",
      "Actor loss: 2.275892734527588\n",
      "Critic loss: 3.8179948329925537\n",
      "Entropy loss: -0.011478076688945293\n",
      "Loss: 6.082409381866455\n",
      "Actor loss: 1.9126625061035156\n",
      "Critic loss: 2.9025158882141113\n",
      "Entropy loss: -0.012964049354195595\n",
      "Loss: 4.8022141456604\n",
      "Actor loss: -0.6633075475692749\n",
      "Critic loss: 0.3061542510986328\n",
      "Entropy loss: -0.012927576899528503\n",
      "Loss: -0.3700808882713318\n",
      "Actor loss: -2.1752920150756836\n",
      "Critic loss: 3.3043177127838135\n",
      "Entropy loss: -0.013066533021628857\n",
      "Loss: 1.1159591674804688\n",
      "Actor loss: 0.29045599699020386\n",
      "Critic loss: 0.08607427030801773\n",
      "Entropy loss: -0.011586662381887436\n",
      "Loss: 0.36494359374046326\n",
      "Actor loss: -0.047661375254392624\n",
      "Critic loss: 0.0011728261597454548\n",
      "Entropy loss: -0.012127748690545559\n",
      "Loss: -0.05861629918217659\n",
      "Actor loss: 0.6135618090629578\n",
      "Critic loss: 0.25128960609436035\n",
      "Entropy loss: -0.012777460739016533\n",
      "Loss: 0.8520739674568176\n",
      "Actor loss: -1.5323623418807983\n",
      "Critic loss: 2.41412353515625\n",
      "Entropy loss: -0.012842084281146526\n",
      "Loss: 0.8689191341400146\n",
      "Actor loss: -2.562150478363037\n",
      "Critic loss: 2.979473829269409\n",
      "Entropy loss: -0.0115667087957263\n",
      "Loss: 0.4057566523551941\n",
      "Actor loss: -1.6031594276428223\n",
      "Critic loss: 1.473787546157837\n",
      "Entropy loss: -0.013121096417307854\n",
      "Loss: -0.14249297976493835\n",
      "Actor loss: 0.2441619485616684\n",
      "Critic loss: 0.03496239334344864\n",
      "Entropy loss: -0.012921208515763283\n",
      "Loss: 0.2662031352519989\n",
      "Actor loss: -0.6897451281547546\n",
      "Critic loss: 0.0955052524805069\n",
      "Entropy loss: -0.012542756274342537\n",
      "Loss: -0.6067826747894287\n",
      "Actor loss: 0.1912936419248581\n",
      "Critic loss: 0.016671188175678253\n",
      "Entropy loss: -0.013432148844003677\n",
      "Loss: 0.19453269243240356\n",
      "Actor loss: -0.9437124133110046\n",
      "Critic loss: 0.48078715801239014\n",
      "Entropy loss: -0.012117202393710613\n",
      "Loss: -0.475042462348938\n",
      "Actor loss: -5.873592376708984\n",
      "Critic loss: 12.798660278320312\n",
      "Entropy loss: -0.011535584926605225\n",
      "Loss: 6.913532257080078\n",
      "Actor loss: 2.455136775970459\n",
      "Critic loss: 2.0235297679901123\n",
      "Entropy loss: -0.013463976792991161\n",
      "Loss: 4.465202331542969\n",
      "Actor loss: 0.7550234794616699\n",
      "Critic loss: 0.5179799199104309\n",
      "Entropy loss: -0.012190742418169975\n",
      "Loss: 1.2608126401901245\n",
      "Actor loss: -2.2749171257019043\n",
      "Critic loss: 3.553966999053955\n",
      "Entropy loss: -0.012748952955007553\n",
      "Loss: 1.266300916671753\n",
      "Actor loss: -1.015625238418579\n",
      "Critic loss: 0.2130066156387329\n",
      "Entropy loss: -0.010981167666614056\n",
      "Loss: -0.8135997653007507\n",
      "Actor loss: -2.533594846725464\n",
      "Critic loss: 3.0706005096435547\n",
      "Entropy loss: -0.013021859340369701\n",
      "Loss: 0.5239837765693665\n",
      "Actor loss: 1.3600966930389404\n",
      "Critic loss: 1.1787328720092773\n",
      "Entropy loss: -0.01232725940644741\n",
      "Loss: 2.5265023708343506\n",
      "Actor loss: -6.453765392303467\n",
      "Critic loss: 9.885299682617188\n",
      "Entropy loss: -0.011199964210391045\n",
      "Loss: 3.4203343391418457\n",
      "Actor loss: -0.5846289396286011\n",
      "Critic loss: 0.2367369681596756\n",
      "Entropy loss: -0.012844052165746689\n",
      "Loss: -0.36073604226112366\n",
      "Actor loss: -0.363902747631073\n",
      "Critic loss: 0.0351974219083786\n",
      "Entropy loss: -0.00991253461688757\n",
      "Loss: -0.33861783146858215\n",
      "Actor loss: -0.4406987130641937\n",
      "Critic loss: 0.06531239300966263\n",
      "Entropy loss: -0.012780562043190002\n",
      "Loss: -0.3881669044494629\n",
      "Actor loss: 1.373483419418335\n",
      "Critic loss: 1.9348618984222412\n",
      "Entropy loss: -0.012461821548640728\n",
      "Loss: 3.2958834171295166\n",
      "Actor loss: -2.1701536178588867\n",
      "Critic loss: 4.609659671783447\n",
      "Entropy loss: -0.009250403381884098\n",
      "Loss: 2.430255651473999\n",
      "Actor loss: -9.1796293258667\n",
      "Critic loss: 8.184524536132812\n",
      "Entropy loss: -0.009151154197752476\n",
      "Loss: -1.0042558908462524\n",
      "Actor loss: 4.050457000732422\n",
      "Critic loss: 2.569244861602783\n",
      "Entropy loss: -0.012059932574629784\n",
      "Loss: 6.607641696929932\n",
      "Actor loss: -2.1913681030273438\n",
      "Critic loss: 1.6718082427978516\n",
      "Entropy loss: -0.011943544261157513\n",
      "Loss: -0.5315033793449402\n",
      "Actor loss: -8.695782661437988\n",
      "Critic loss: 5.272279262542725\n",
      "Entropy loss: -0.010558164678514004\n",
      "Loss: -3.4340615272521973\n",
      "Actor loss: -2.3601431846618652\n",
      "Critic loss: 0.8708940744400024\n",
      "Entropy loss: -0.012041576206684113\n",
      "Loss: -1.5012906789779663\n",
      "Actor loss: -0.009898917749524117\n",
      "Critic loss: 1.8895341781899333e-05\n",
      "Entropy loss: -0.011804807931184769\n",
      "Loss: -0.021684829145669937\n",
      "Actor loss: 4.869143962860107\n",
      "Critic loss: 2.6614367961883545\n",
      "Entropy loss: -0.009753554128110409\n",
      "Loss: 7.520826816558838\n",
      "Actor loss: -1.188778281211853\n",
      "Critic loss: 0.8935673832893372\n",
      "Entropy loss: -0.009712748229503632\n",
      "Loss: -0.3049236536026001\n",
      "Actor loss: 1.1456135511398315\n",
      "Critic loss: 0.7428867816925049\n",
      "Entropy loss: -0.008995176292955875\n",
      "Loss: 1.8795051574707031\n",
      "Actor loss: 2.722108840942383\n",
      "Critic loss: 1.2143110036849976\n",
      "Entropy loss: -0.009961039759218693\n",
      "Loss: 3.9264588356018066\n",
      "Actor loss: 5.613329887390137\n",
      "Critic loss: 1.7203282117843628\n",
      "Entropy loss: -0.008493456989526749\n",
      "Loss: 7.325164794921875\n",
      "Actor loss: 10.576865196228027\n",
      "Critic loss: 4.075717926025391\n",
      "Entropy loss: -0.00967351347208023\n",
      "Loss: 14.64291000366211\n",
      "Actor loss: 0.6913003921508789\n",
      "Critic loss: 0.16506265103816986\n",
      "Entropy loss: -0.010382140055298805\n",
      "Loss: 0.8459809422492981\n",
      "Actor loss: -1.6125333309173584\n",
      "Critic loss: 0.47803905606269836\n",
      "Entropy loss: -0.009524093009531498\n",
      "Loss: -1.1440184116363525\n",
      "Actor loss: 15.437655448913574\n",
      "Critic loss: 2.892336368560791\n",
      "Entropy loss: -0.008935337886214256\n",
      "Loss: 18.321056365966797\n",
      "Actor loss: -1.12733793258667\n",
      "Critic loss: 1.1606707572937012\n",
      "Entropy loss: -0.012724730186164379\n",
      "Loss: 0.020608093589544296\n",
      "Actor loss: 2.976799726486206\n",
      "Critic loss: 0.47814661264419556\n",
      "Entropy loss: -0.008705331943929195\n",
      "Loss: 3.4462409019470215\n",
      "Actor loss: -0.4118483066558838\n",
      "Critic loss: 0.12473200261592865\n",
      "Entropy loss: -0.012579871341586113\n",
      "Loss: -0.299696147441864\n",
      "Actor loss: 2.7192916870117188\n",
      "Critic loss: 3.505514144897461\n",
      "Entropy loss: -0.011008812114596367\n",
      "Loss: 6.213797092437744\n",
      "Actor loss: 0.6810700297355652\n",
      "Critic loss: 0.322915643453598\n",
      "Entropy loss: -0.011630749329924583\n",
      "Loss: 0.9923548698425293\n",
      "Actor loss: 1.8047595024108887\n",
      "Critic loss: 1.5130959749221802\n",
      "Entropy loss: -0.012651417404413223\n",
      "Loss: 3.305203914642334\n",
      "Actor loss: -0.37634843587875366\n",
      "Critic loss: 0.13121415674686432\n",
      "Entropy loss: -0.013693751767277718\n",
      "Loss: -0.2588280439376831\n",
      "Actor loss: -0.7307723760604858\n",
      "Critic loss: 0.2668471932411194\n",
      "Entropy loss: -0.01165076531469822\n",
      "Loss: -0.4755759537220001\n",
      "Actor loss: 3.9954304695129395\n",
      "Critic loss: 3.9016530513763428\n",
      "Entropy loss: -0.012760418467223644\n",
      "Loss: 7.884322643280029\n",
      "Actor loss: 1.2906239032745361\n",
      "Critic loss: 1.1905152797698975\n",
      "Entropy loss: -0.013037930242717266\n",
      "Loss: 2.4681012630462646\n",
      "Actor loss: 0.12230826914310455\n",
      "Critic loss: 0.012027387507259846\n",
      "Entropy loss: -0.01352015696465969\n",
      "Loss: 0.12081549316644669\n",
      "Actor loss: -0.7798938751220703\n",
      "Critic loss: 0.4738476574420929\n",
      "Entropy loss: -0.011086929589509964\n",
      "Loss: -0.3171331584453583\n",
      "Actor loss: 0.6539660692214966\n",
      "Critic loss: 0.42482075095176697\n",
      "Entropy loss: -0.012493809685111046\n",
      "Loss: 1.0662930011749268\n",
      "Actor loss: 0.0441456064581871\n",
      "Critic loss: 0.001676111831329763\n",
      "Entropy loss: -0.013472642749547958\n",
      "Loss: 0.03234907612204552\n",
      "Actor loss: 0.26696398854255676\n",
      "Critic loss: 0.02528234012424946\n",
      "Entropy loss: -0.012226677499711514\n",
      "Loss: 0.2800196707248688\n",
      "Actor loss: -2.013746976852417\n",
      "Critic loss: 0.9371703267097473\n",
      "Entropy loss: -0.014598214998841286\n",
      "Loss: -1.0911749601364136\n",
      "Actor loss: 2.3602590560913086\n",
      "Critic loss: 4.7256622314453125\n",
      "Entropy loss: -0.013323510065674782\n",
      "Loss: 7.072597980499268\n",
      "Actor loss: -4.141709327697754\n",
      "Critic loss: 2.2000200748443604\n",
      "Entropy loss: -0.010372305288910866\n",
      "Loss: -1.9520615339279175\n",
      "Actor loss: -0.29593417048454285\n",
      "Critic loss: 0.06265417486429214\n",
      "Entropy loss: -0.013496868312358856\n",
      "Loss: -0.24677687883377075\n",
      "Actor loss: -1.5307056903839111\n",
      "Critic loss: 0.7970772385597229\n",
      "Entropy loss: -0.01352138351649046\n",
      "Loss: -0.7471498250961304\n",
      "Actor loss: 0.2790087163448334\n",
      "Critic loss: 0.01636396162211895\n",
      "Entropy loss: -0.013939835131168365\n",
      "Loss: 0.2814328372478485\n",
      "Actor loss: 0.6052114367485046\n",
      "Critic loss: 0.2103104442358017\n",
      "Entropy loss: -0.01273922249674797\n",
      "Loss: 0.8027826547622681\n",
      "Actor loss: 0.22013628482818604\n",
      "Critic loss: 0.023452792316675186\n",
      "Entropy loss: -0.014371560886502266\n",
      "Loss: 0.2292175143957138\n",
      "Actor loss: -1.4033515453338623\n",
      "Critic loss: 0.7943305969238281\n",
      "Entropy loss: -0.01508953608572483\n",
      "Loss: -0.6241104602813721\n",
      "Actor loss: 1.1179440021514893\n",
      "Critic loss: 1.5065574645996094\n",
      "Entropy loss: -0.013628448359668255\n",
      "Loss: 2.610872983932495\n",
      "Actor loss: -0.8889656066894531\n",
      "Critic loss: 0.060059674084186554\n",
      "Entropy loss: -0.008593843318521976\n",
      "Loss: -0.8374997973442078\n",
      "Actor loss: -1.3326467275619507\n",
      "Critic loss: 0.4413628578186035\n",
      "Entropy loss: -0.014953045174479485\n",
      "Loss: -0.9062368869781494\n",
      "Actor loss: -0.876349151134491\n",
      "Critic loss: 0.16747424006462097\n",
      "Entropy loss: -0.013049580156803131\n",
      "Loss: -0.7219245433807373\n",
      "Actor loss: 0.753661572933197\n",
      "Critic loss: 0.32349443435668945\n",
      "Entropy loss: -0.01166912354528904\n",
      "Loss: 1.0654869079589844\n",
      "Actor loss: 0.7220847010612488\n",
      "Critic loss: 0.1899455189704895\n",
      "Entropy loss: -0.013929969631135464\n",
      "Loss: 0.8981002569198608\n",
      "Actor loss: 0.7462248802185059\n",
      "Critic loss: 0.6130488514900208\n",
      "Entropy loss: -0.012316781096160412\n",
      "Loss: 1.3469568490982056\n",
      "Actor loss: 0.5079492330551147\n",
      "Critic loss: 0.05796685442328453\n",
      "Entropy loss: -0.011983594857156277\n",
      "Loss: 0.5539324879646301\n",
      "Actor loss: -0.9152183532714844\n",
      "Critic loss: 0.24702255427837372\n",
      "Entropy loss: -0.01505211926996708\n",
      "Loss: -0.6832479238510132\n",
      "Actor loss: 0.7774490118026733\n",
      "Critic loss: 0.21823778748512268\n",
      "Entropy loss: -0.0135599784553051\n",
      "Loss: 0.982126772403717\n",
      "Actor loss: 0.26934486627578735\n",
      "Critic loss: 0.01686890982091427\n",
      "Entropy loss: -0.012035433202981949\n",
      "Loss: 0.2741783559322357\n",
      "Actor loss: -0.2825455665588379\n",
      "Critic loss: 0.04739575833082199\n",
      "Entropy loss: -0.013618675991892815\n",
      "Loss: -0.24876847863197327\n",
      "Actor loss: 2.9100189208984375\n",
      "Critic loss: 2.510343074798584\n",
      "Entropy loss: -0.01270860806107521\n",
      "Loss: 5.407653331756592\n",
      "Actor loss: 1.5822186470031738\n",
      "Critic loss: 0.5381938219070435\n",
      "Entropy loss: -0.011685649864375591\n",
      "Loss: 2.108726739883423\n",
      "Actor loss: -0.1723480075597763\n",
      "Critic loss: 0.013410142622888088\n",
      "Entropy loss: -0.012636413797736168\n",
      "Loss: -0.17157427966594696\n",
      "Actor loss: -0.37791675329208374\n",
      "Critic loss: 0.037652354687452316\n",
      "Entropy loss: -0.01529845129698515\n",
      "Loss: -0.35556286573410034\n",
      "Actor loss: 0.32743120193481445\n",
      "Critic loss: 0.08015185594558716\n",
      "Entropy loss: -0.013625286519527435\n",
      "Loss: 0.3939577639102936\n",
      "Actor loss: 0.8139419555664062\n",
      "Critic loss: 0.36486974358558655\n",
      "Entropy loss: -0.012652218341827393\n",
      "Loss: 1.1661593914031982\n",
      "Actor loss: -0.5634952783584595\n",
      "Critic loss: 0.07149955630302429\n",
      "Entropy loss: -0.01247414480894804\n",
      "Loss: -0.5044698715209961\n",
      "Actor loss: 0.48162028193473816\n",
      "Critic loss: 0.08965887129306793\n",
      "Entropy loss: -0.014329264871776104\n",
      "Loss: 0.5569499135017395\n",
      "Actor loss: 0.5656446814537048\n",
      "Critic loss: 0.2888706624507904\n",
      "Entropy loss: -0.013241982087492943\n",
      "Loss: 0.841273307800293\n",
      "Actor loss: 0.3570440411567688\n",
      "Critic loss: 0.08942332863807678\n",
      "Entropy loss: -0.011867158114910126\n",
      "Loss: 0.43460020422935486\n",
      "Actor loss: 0.8059827089309692\n",
      "Critic loss: 0.30230119824409485\n",
      "Entropy loss: -0.012104954570531845\n",
      "Loss: 1.0961788892745972\n",
      "Actor loss: -0.9288128614425659\n",
      "Critic loss: 0.6648257374763489\n",
      "Entropy loss: -0.014816444367170334\n",
      "Loss: -0.2788035571575165\n",
      "Actor loss: 0.33241888880729675\n",
      "Critic loss: 0.16786931455135345\n",
      "Entropy loss: -0.010588405653834343\n",
      "Loss: 0.4896997809410095\n",
      "Actor loss: 0.4071098268032074\n",
      "Critic loss: 0.13345451653003693\n",
      "Entropy loss: -0.012564490549266338\n",
      "Loss: 0.5279998779296875\n",
      "Actor loss: -0.025011878460645676\n",
      "Critic loss: 0.0003535842115525156\n",
      "Entropy loss: -0.01440788060426712\n",
      "Loss: -0.03906617313623428\n",
      "Actor loss: -0.7717799544334412\n",
      "Critic loss: 0.3932076096534729\n",
      "Entropy loss: -0.012449483387172222\n",
      "Loss: -0.39102181792259216\n",
      "Actor loss: 0.535266637802124\n",
      "Critic loss: 0.23055213689804077\n",
      "Entropy loss: -0.014106385409832\n",
      "Loss: 0.7517123818397522\n",
      "Actor loss: -0.9646083116531372\n",
      "Critic loss: 0.27498459815979004\n",
      "Entropy loss: -0.011072441004216671\n",
      "Loss: -0.7006961703300476\n",
      "Actor loss: -2.513941764831543\n",
      "Critic loss: 1.7658648490905762\n",
      "Entropy loss: -0.01481259148567915\n",
      "Loss: -0.7628895044326782\n",
      "Actor loss: 0.12111362814903259\n",
      "Critic loss: 0.012377460487186909\n",
      "Entropy loss: -0.014556802809238434\n",
      "Loss: 0.1189342811703682\n",
      "Actor loss: -0.49935731291770935\n",
      "Critic loss: 0.13567671179771423\n",
      "Entropy loss: -0.012671038508415222\n",
      "Loss: -0.37635165452957153\n",
      "Actor loss: 0.3699686527252197\n",
      "Critic loss: 0.06678304821252823\n",
      "Entropy loss: -0.012972228229045868\n",
      "Loss: 0.4237794578075409\n",
      "Actor loss: 0.1838395595550537\n",
      "Critic loss: 0.02647578902542591\n",
      "Entropy loss: -0.013577314093708992\n",
      "Loss: 0.19673803448677063\n",
      "Actor loss: -1.6540454626083374\n",
      "Critic loss: 0.9277655482292175\n",
      "Entropy loss: -0.010928213596343994\n",
      "Loss: -0.7372081279754639\n",
      "Actor loss: -0.5131916999816895\n",
      "Critic loss: 0.13512364029884338\n",
      "Entropy loss: -0.014466425403952599\n",
      "Loss: -0.3925344944000244\n",
      "Actor loss: 0.3134303689002991\n",
      "Critic loss: 0.035643529146909714\n",
      "Entropy loss: -0.011997487396001816\n",
      "Loss: 0.3370763957500458\n",
      "Actor loss: -1.616764783859253\n",
      "Critic loss: 0.8233245015144348\n",
      "Entropy loss: -0.011625881306827068\n",
      "Loss: -0.8050661683082581\n",
      "Actor loss: -0.0927167534828186\n",
      "Critic loss: 0.007708635181188583\n",
      "Entropy loss: -0.014423659071326256\n",
      "Loss: -0.09943177551031113\n",
      "Actor loss: -0.1402820199728012\n",
      "Critic loss: 0.011858498677611351\n",
      "Entropy loss: -0.011615322902798653\n",
      "Loss: -0.1400388479232788\n",
      "Actor loss: -1.8159167766571045\n",
      "Critic loss: 1.3951568603515625\n",
      "Entropy loss: -0.011923848651349545\n",
      "Loss: -0.4326837658882141\n",
      "Actor loss: 0.4973522126674652\n",
      "Critic loss: 0.12164904922246933\n",
      "Entropy loss: -0.013940980657935143\n",
      "Loss: 0.6050602793693542\n",
      "Actor loss: 0.259101927280426\n",
      "Critic loss: 0.04167984798550606\n",
      "Entropy loss: -0.012682276777923107\n",
      "Loss: 0.2880994975566864\n",
      "Actor loss: 0.6126371622085571\n",
      "Critic loss: 0.271840900182724\n",
      "Entropy loss: -0.012334860861301422\n",
      "Loss: 0.8721432089805603\n",
      "Actor loss: -0.005657299421727657\n",
      "Critic loss: 1.0554771506576799e-05\n",
      "Entropy loss: -0.015103926882147789\n",
      "Loss: -0.02075067162513733\n",
      "Actor loss: -0.18999207019805908\n",
      "Critic loss: 0.012989713810384274\n",
      "Entropy loss: -0.013794551603496075\n",
      "Loss: -0.19079691171646118\n",
      "Actor loss: 0.6221065521240234\n",
      "Critic loss: 0.12131956219673157\n",
      "Entropy loss: -0.012015610001981258\n",
      "Loss: 0.7314105033874512\n",
      "Actor loss: -0.2168547511100769\n",
      "Critic loss: 0.015021595172584057\n",
      "Entropy loss: -0.013180888257920742\n",
      "Loss: -0.2150140404701233\n",
      "Actor loss: -0.6734429597854614\n",
      "Critic loss: 0.21123936772346497\n",
      "Entropy loss: -0.01559282373636961\n",
      "Loss: -0.47779640555381775\n",
      "Actor loss: 0.7152311205863953\n",
      "Critic loss: 0.11736001074314117\n",
      "Entropy loss: -0.012179742567241192\n",
      "Loss: 0.8204113841056824\n",
      "Actor loss: 2.0726540088653564\n",
      "Critic loss: 1.390974760055542\n",
      "Entropy loss: -0.012415794655680656\n",
      "Loss: 3.4512128829956055\n",
      "Actor loss: -0.3397352695465088\n",
      "Critic loss: 0.02181202732026577\n",
      "Entropy loss: -0.013621619902551174\n",
      "Loss: -0.3315448760986328\n",
      "Actor loss: 1.3567659854888916\n",
      "Critic loss: 1.6382704973220825\n",
      "Entropy loss: -0.013393059372901917\n",
      "Loss: 2.9816434383392334\n",
      "Actor loss: 7.997146129608154\n",
      "Critic loss: 0.4506162405014038\n",
      "Entropy loss: -0.00780398678034544\n",
      "Loss: 8.439958572387695\n",
      "Actor loss: 1.7008088827133179\n",
      "Critic loss: 0.5943312048912048\n",
      "Entropy loss: -0.014751212671399117\n",
      "Loss: 2.280388832092285\n",
      "Actor loss: 1.3612383604049683\n",
      "Critic loss: 0.9751793742179871\n",
      "Entropy loss: -0.014162881299853325\n",
      "Loss: 2.3222548961639404\n",
      "Actor loss: -0.6205100417137146\n",
      "Critic loss: 0.14667952060699463\n",
      "Entropy loss: -0.013423353433609009\n",
      "Loss: -0.487253874540329\n",
      "Actor loss: -0.43266528844833374\n",
      "Critic loss: 0.08796802908182144\n",
      "Entropy loss: -0.013331900350749493\n",
      "Loss: -0.35802915692329407\n",
      "Actor loss: -0.05821993201971054\n",
      "Critic loss: 0.0025533849839121103\n",
      "Entropy loss: -0.013582907617092133\n",
      "Loss: -0.06924945116043091\n",
      "Actor loss: -0.6286367774009705\n",
      "Critic loss: 0.21357566118240356\n",
      "Entropy loss: -0.013717873021960258\n",
      "Loss: -0.4287789762020111\n",
      "Actor loss: -1.2011969089508057\n",
      "Critic loss: 0.8012625575065613\n",
      "Entropy loss: -0.015223841182887554\n",
      "Loss: -0.4151581823825836\n",
      "Actor loss: 0.47143813967704773\n",
      "Critic loss: 0.18583297729492188\n",
      "Entropy loss: -0.013233627192676067\n",
      "Loss: 0.6440375447273254\n",
      "Actor loss: -1.9532585144042969\n",
      "Critic loss: 0.8234712481498718\n",
      "Entropy loss: -0.012685798108577728\n",
      "Loss: -1.1424729824066162\n",
      "Actor loss: 0.6248855590820312\n",
      "Critic loss: 0.2585044503211975\n",
      "Entropy loss: -0.014074623584747314\n",
      "Loss: 0.8693153858184814\n",
      "Actor loss: -0.16143406927585602\n",
      "Critic loss: 0.009193312376737595\n",
      "Entropy loss: -0.01369781419634819\n",
      "Loss: -0.1659385710954666\n",
      "Actor loss: 0.2854907214641571\n",
      "Critic loss: 0.03753982111811638\n",
      "Entropy loss: -0.012277158908545971\n",
      "Loss: 0.31075337529182434\n",
      "Actor loss: -0.12398412078619003\n",
      "Critic loss: 0.005939155351370573\n",
      "Entropy loss: -0.014043984934687614\n",
      "Loss: -0.13208894431591034\n",
      "Actor loss: 0.9925119280815125\n",
      "Critic loss: 1.0266711711883545\n",
      "Entropy loss: -0.012286357581615448\n",
      "Loss: 2.006896734237671\n",
      "Actor loss: -1.9999570846557617\n",
      "Critic loss: 1.3411028385162354\n",
      "Entropy loss: -0.014340932480990887\n",
      "Loss: -0.6731951832771301\n",
      "Actor loss: 0.8961848020553589\n",
      "Critic loss: 0.48428115248680115\n",
      "Entropy loss: -0.014512437395751476\n",
      "Loss: 1.3659535646438599\n",
      "Actor loss: -0.8645392656326294\n",
      "Critic loss: 0.4066919684410095\n",
      "Entropy loss: -0.012557556852698326\n",
      "Loss: -0.47040486335754395\n",
      "Actor loss: -0.30579206347465515\n",
      "Critic loss: 0.06496713310480118\n",
      "Entropy loss: -0.013090471737086773\n",
      "Loss: -0.253915399312973\n",
      "Actor loss: -0.6164124608039856\n",
      "Critic loss: 0.25695371627807617\n",
      "Entropy loss: -0.013683761470019817\n",
      "Loss: -0.3731425106525421\n",
      "Actor loss: -0.7797837257385254\n",
      "Critic loss: 0.1975841373205185\n",
      "Entropy loss: -0.014059221372008324\n",
      "Loss: -0.596258819103241\n",
      "Actor loss: 0.31342464685440063\n",
      "Critic loss: 0.05889274924993515\n",
      "Entropy loss: -0.014165005646646023\n",
      "Loss: 0.3581523895263672\n",
      "Actor loss: -0.7058171629905701\n",
      "Critic loss: 0.2813795208930969\n",
      "Entropy loss: -0.01293947733938694\n",
      "Loss: -0.43737712502479553\n",
      "Actor loss: -2.062272548675537\n",
      "Critic loss: 2.178147315979004\n",
      "Entropy loss: -0.013700245879590511\n",
      "Loss: 0.10217452049255371\n",
      "Actor loss: 0.3785322308540344\n",
      "Critic loss: 0.07893374562263489\n",
      "Entropy loss: -0.015880662947893143\n",
      "Loss: 0.4415853023529053\n",
      "Actor loss: 0.08391749858856201\n",
      "Critic loss: 0.0052273268811404705\n",
      "Entropy loss: -0.012461821548640728\n",
      "Loss: 0.07668300718069077\n",
      "Actor loss: -0.8336000442504883\n",
      "Critic loss: 0.3113272488117218\n",
      "Entropy loss: -0.014911039732396603\n",
      "Loss: -0.5371838808059692\n",
      "Actor loss: -1.4454345703125\n",
      "Critic loss: 0.7788246273994446\n",
      "Entropy loss: -0.012797584757208824\n",
      "Loss: -0.67940753698349\n",
      "Actor loss: -0.12310749292373657\n",
      "Critic loss: 0.010128148831427097\n",
      "Entropy loss: -0.014837883412837982\n",
      "Loss: -0.12781722843647003\n",
      "Actor loss: -0.8949088454246521\n",
      "Critic loss: 0.3730979263782501\n",
      "Entropy loss: -0.012444622814655304\n",
      "Loss: -0.5342555046081543\n",
      "Actor loss: 0.31917160749435425\n",
      "Critic loss: 0.05249873921275139\n",
      "Entropy loss: -0.014433581382036209\n",
      "Loss: 0.35723674297332764\n",
      "Actor loss: -1.369559645652771\n",
      "Critic loss: 0.4064611792564392\n",
      "Entropy loss: -0.011558844707906246\n",
      "Loss: -0.9746572971343994\n",
      "Actor loss: -0.7332324385643005\n",
      "Critic loss: 0.5067281126976013\n",
      "Entropy loss: -0.012861630879342556\n",
      "Loss: -0.23936595022678375\n",
      "Actor loss: -1.308013677597046\n",
      "Critic loss: 0.8273144960403442\n",
      "Entropy loss: -0.01580945774912834\n",
      "Loss: -0.4965086281299591\n",
      "Actor loss: 1.4277738332748413\n",
      "Critic loss: 0.8656628131866455\n",
      "Entropy loss: -0.01356489211320877\n",
      "Loss: 2.279871702194214\n",
      "Actor loss: -1.2271027565002441\n",
      "Critic loss: 0.19792532920837402\n",
      "Entropy loss: -0.010543721728026867\n",
      "Loss: -1.039721131324768\n",
      "Actor loss: -0.9504754543304443\n",
      "Critic loss: 0.38278648257255554\n",
      "Entropy loss: -0.016329703852534294\n",
      "Loss: -0.5840186476707458\n",
      "Actor loss: -0.1343308985233307\n",
      "Critic loss: 0.007937993854284286\n",
      "Entropy loss: -0.013845540583133698\n",
      "Loss: -0.1402384340763092\n",
      "Actor loss: -2.1174936294555664\n",
      "Critic loss: 2.2217671871185303\n",
      "Entropy loss: -0.013263121247291565\n",
      "Loss: 0.0910104364156723\n",
      "Actor loss: -0.2476164549589157\n",
      "Critic loss: 0.0442410372197628\n",
      "Entropy loss: -0.014558035880327225\n",
      "Loss: -0.21793344616889954\n",
      "Actor loss: 0.20042382180690765\n",
      "Critic loss: 0.03428247570991516\n",
      "Entropy loss: -0.014166498556733131\n",
      "Loss: 0.22053979337215424\n",
      "Actor loss: -1.0279921293258667\n",
      "Critic loss: 0.33931204676628113\n",
      "Entropy loss: -0.011831949464976788\n",
      "Loss: -0.7005119919776917\n",
      "Actor loss: -0.3068370819091797\n",
      "Critic loss: 0.06572055071592331\n",
      "Entropy loss: -0.013881666585803032\n",
      "Loss: -0.25499817728996277\n",
      "Actor loss: -0.7544962167739868\n",
      "Critic loss: 0.1614207625389099\n",
      "Entropy loss: -0.015542658977210522\n",
      "Loss: -0.6086181402206421\n",
      "Actor loss: 0.3793807625770569\n",
      "Critic loss: 0.0697416365146637\n",
      "Entropy loss: -0.01369363721460104\n",
      "Loss: 0.43542876839637756\n",
      "Actor loss: -0.40885528922080994\n",
      "Critic loss: 0.11178158223628998\n",
      "Entropy loss: -0.0141958799213171\n",
      "Loss: -0.311269611120224\n",
      "Actor loss: 0.830624520778656\n",
      "Critic loss: 0.1624126434326172\n",
      "Entropy loss: -0.012280675582587719\n",
      "Loss: 0.9807564616203308\n",
      "Actor loss: 0.13012124598026276\n",
      "Critic loss: 0.004033065866678953\n",
      "Entropy loss: -0.012129123322665691\n",
      "Loss: 0.12202518433332443\n",
      "Actor loss: 0.8734418749809265\n",
      "Critic loss: 0.3571016490459442\n",
      "Entropy loss: -0.014313971623778343\n",
      "Loss: 1.2162295579910278\n",
      "Actor loss: 0.6440345048904419\n",
      "Critic loss: 0.09873253107070923\n",
      "Entropy loss: -0.01363137923181057\n",
      "Loss: 0.7291356325149536\n",
      "Actor loss: 0.98888099193573\n",
      "Critic loss: 0.8350314497947693\n",
      "Entropy loss: -0.013042272999882698\n",
      "Loss: 1.8108700513839722\n",
      "Actor loss: 0.733279824256897\n",
      "Critic loss: 0.12750346958637238\n",
      "Entropy loss: -0.013464885763823986\n",
      "Loss: 0.8473184108734131\n",
      "Actor loss: 0.1551702618598938\n",
      "Critic loss: 0.014106950722634792\n",
      "Entropy loss: -0.01251404732465744\n",
      "Loss: 0.15676316618919373\n",
      "Actor loss: 0.43937021493911743\n",
      "Critic loss: 0.13035452365875244\n",
      "Entropy loss: -0.014284450560808182\n",
      "Loss: 0.5554403066635132\n",
      "Actor loss: 0.23806528747081757\n",
      "Critic loss: 0.028566671535372734\n",
      "Entropy loss: -0.013686971738934517\n",
      "Loss: 0.2529449760913849\n",
      "Actor loss: 0.43440017104148865\n",
      "Critic loss: 0.06631425768136978\n",
      "Entropy loss: -0.01354562770575285\n",
      "Loss: 0.4871687889099121\n",
      "Actor loss: -0.7823302149772644\n",
      "Critic loss: 0.25254613161087036\n",
      "Entropy loss: -0.014065064489841461\n",
      "Loss: -0.5438491702079773\n",
      "Actor loss: 0.29364460706710815\n",
      "Critic loss: 0.0790964663028717\n",
      "Entropy loss: -0.01378590613603592\n",
      "Loss: 0.35895517468452454\n",
      "Actor loss: -0.19718876481056213\n",
      "Critic loss: 0.004697695840150118\n",
      "Entropy loss: -0.010247517377138138\n",
      "Loss: -0.20273858308792114\n",
      "Actor loss: -0.4240947663784027\n",
      "Critic loss: 0.08017221093177795\n",
      "Entropy loss: -0.015542557463049889\n",
      "Loss: -0.3594651222229004\n",
      "Actor loss: 2.851524829864502\n",
      "Critic loss: 0.9151433110237122\n",
      "Entropy loss: -0.010400848463177681\n",
      "Loss: 3.7562673091888428\n",
      "Actor loss: 1.2128156423568726\n",
      "Critic loss: 0.6524132490158081\n",
      "Entropy loss: -0.014774876646697521\n",
      "Loss: 1.8504539728164673\n",
      "Actor loss: 0.8871163725852966\n",
      "Critic loss: 0.09410812705755234\n",
      "Entropy loss: -0.010418339632451534\n",
      "Loss: 0.9708061218261719\n",
      "Actor loss: -0.2721601128578186\n",
      "Critic loss: 0.02538437582552433\n",
      "Entropy loss: -0.014968812465667725\n",
      "Loss: -0.26174455881118774\n",
      "Actor loss: 2.3198699951171875\n",
      "Critic loss: 3.222215175628662\n",
      "Entropy loss: -0.01401305478066206\n",
      "Loss: 5.528071880340576\n",
      "Actor loss: -1.342763900756836\n",
      "Critic loss: 0.4906768798828125\n",
      "Entropy loss: -0.012466873973608017\n",
      "Loss: -0.8645538687705994\n",
      "Actor loss: 1.0592725276947021\n",
      "Critic loss: 0.7301314473152161\n",
      "Entropy loss: -0.01615772396326065\n",
      "Loss: 1.773246169090271\n",
      "Actor loss: 1.8388570547103882\n",
      "Critic loss: 0.7398502230644226\n",
      "Entropy loss: -0.012049424462020397\n",
      "Loss: 2.566657781600952\n",
      "Actor loss: 1.3369150161743164\n",
      "Critic loss: 1.0387418270111084\n",
      "Entropy loss: -0.012205004692077637\n",
      "Loss: 2.3634519577026367\n",
      "Actor loss: 0.5247207880020142\n",
      "Critic loss: 0.1999882161617279\n",
      "Entropy loss: -0.01282444130629301\n",
      "Loss: 0.711884617805481\n",
      "Actor loss: 0.9403767585754395\n",
      "Critic loss: 0.42103955149650574\n",
      "Entropy loss: -0.014656811952590942\n",
      "Loss: 1.346759557723999\n",
      "Actor loss: 1.0335345268249512\n",
      "Critic loss: 0.536702036857605\n",
      "Entropy loss: -0.01362868957221508\n",
      "Loss: 1.5566078424453735\n",
      "Actor loss: 0.9666566252708435\n",
      "Critic loss: 0.7203496694564819\n",
      "Entropy loss: -0.014314590021967888\n",
      "Loss: 1.6726917028427124\n",
      "Actor loss: 0.4628257751464844\n",
      "Critic loss: 0.10058716684579849\n",
      "Entropy loss: -0.012303486466407776\n",
      "Loss: 0.5511094927787781\n",
      "Actor loss: 0.7217671275138855\n",
      "Critic loss: 0.4988526403903961\n",
      "Entropy loss: -0.013308647088706493\n",
      "Loss: 1.2073111534118652\n",
      "Actor loss: 0.17696283757686615\n",
      "Critic loss: 0.011079645715653896\n",
      "Entropy loss: -0.012943646870553493\n",
      "Loss: 0.17509883642196655\n",
      "Actor loss: 0.5838019847869873\n",
      "Critic loss: 0.16364602744579315\n",
      "Entropy loss: -0.014973711222410202\n",
      "Loss: 0.7324743270874023\n",
      "Actor loss: 0.9380217790603638\n",
      "Critic loss: 1.182340145111084\n",
      "Entropy loss: -0.013157798908650875\n",
      "Loss: 2.107203960418701\n",
      "Actor loss: -0.4039781987667084\n",
      "Critic loss: 0.03564683720469475\n",
      "Entropy loss: -0.013251060619950294\n",
      "Loss: -0.38158243894577026\n",
      "Actor loss: -0.869041383266449\n",
      "Critic loss: 0.6412193179130554\n",
      "Entropy loss: -0.014009731821715832\n",
      "Loss: -0.24183179438114166\n",
      "Actor loss: 0.22285757958889008\n",
      "Critic loss: 0.035957593470811844\n",
      "Entropy loss: -0.012459062039852142\n",
      "Loss: 0.24635609984397888\n",
      "Actor loss: -1.4213151931762695\n",
      "Critic loss: 0.6147311925888062\n",
      "Entropy loss: -0.015654686838388443\n",
      "Loss: -0.8222386837005615\n",
      "Actor loss: -0.4808505177497864\n",
      "Critic loss: 0.1260499805212021\n",
      "Entropy loss: -0.013844716362655163\n",
      "Loss: -0.36864525079727173\n",
      "Actor loss: 0.017991669476032257\n",
      "Critic loss: 0.0001747616333886981\n",
      "Entropy loss: -0.013731420040130615\n",
      "Loss: 0.004435010254383087\n",
      "Actor loss: -0.6348826289176941\n",
      "Critic loss: 0.1262381374835968\n",
      "Entropy loss: -0.013169502839446068\n",
      "Loss: -0.5218139886856079\n",
      "Actor loss: 0.16804032027721405\n",
      "Critic loss: 0.032802384346723557\n",
      "Entropy loss: -0.013004216365516186\n",
      "Loss: 0.18783849477767944\n",
      "Actor loss: -0.9712464809417725\n",
      "Critic loss: 0.8296061158180237\n",
      "Entropy loss: -0.014172673225402832\n",
      "Loss: -0.1558130383491516\n",
      "Actor loss: -0.36631613969802856\n",
      "Critic loss: 0.12412340939044952\n",
      "Entropy loss: -0.013813896104693413\n",
      "Loss: -0.2560066282749176\n",
      "Actor loss: -0.5609515905380249\n",
      "Critic loss: 0.281210720539093\n",
      "Entropy loss: -0.013243327848613262\n",
      "Loss: -0.2929841876029968\n",
      "Actor loss: -0.8093745112419128\n",
      "Critic loss: 0.5075097680091858\n",
      "Entropy loss: -0.013721324503421783\n",
      "Loss: -0.31558606028556824\n",
      "Actor loss: 0.2873796224594116\n",
      "Critic loss: 0.06805961579084396\n",
      "Entropy loss: -0.014416702091693878\n",
      "Loss: 0.3410225510597229\n",
      "Actor loss: -0.5958044528961182\n",
      "Critic loss: 0.2794415056705475\n",
      "Entropy loss: -0.012177861295640469\n",
      "Loss: -0.3285408020019531\n",
      "Actor loss: -0.1603846698999405\n",
      "Critic loss: 0.014839588664472103\n",
      "Entropy loss: -0.013186799362301826\n",
      "Loss: -0.1587318778038025\n",
      "Actor loss: 0.11543715000152588\n",
      "Critic loss: 0.011259092949330807\n",
      "Entropy loss: -0.014151882380247116\n",
      "Loss: 0.11254435777664185\n",
      "Actor loss: -1.5122299194335938\n",
      "Critic loss: 0.9509307742118835\n",
      "Entropy loss: -0.013342170976102352\n",
      "Loss: -0.5746412873268127\n",
      "Actor loss: -1.0395772457122803\n",
      "Critic loss: 0.9836460947990417\n",
      "Entropy loss: -0.013333908282220364\n",
      "Loss: -0.06926506012678146\n",
      "Actor loss: -0.9872521758079529\n",
      "Critic loss: 0.5701443552970886\n",
      "Entropy loss: -0.014811499044299126\n",
      "Loss: -0.43191930651664734\n",
      "Actor loss: -0.3021221458911896\n",
      "Critic loss: 0.09117328375577927\n",
      "Entropy loss: -0.013412632048130035\n",
      "Loss: -0.22436147928237915\n",
      "Actor loss: -0.26071327924728394\n",
      "Critic loss: 0.03996085748076439\n",
      "Entropy loss: -0.012807304970920086\n",
      "Loss: -0.2335597276687622\n",
      "Actor loss: -1.9535964727401733\n",
      "Critic loss: 1.9543050527572632\n",
      "Entropy loss: -0.014059755951166153\n",
      "Loss: -0.01335117593407631\n",
      "Actor loss: -0.36465761065483093\n",
      "Critic loss: 0.06359726935625076\n",
      "Entropy loss: -0.012821553274989128\n",
      "Loss: -0.31388190388679504\n",
      "Actor loss: 0.1255406141281128\n",
      "Critic loss: 0.007527809590101242\n",
      "Entropy loss: -0.0150374174118042\n",
      "Loss: 0.11803101003170013\n",
      "Actor loss: -0.4454725980758667\n",
      "Critic loss: 0.09674064069986343\n",
      "Entropy loss: -0.013084456324577332\n",
      "Loss: -0.36181640625\n",
      "Actor loss: -0.5665796399116516\n",
      "Critic loss: 0.19802454113960266\n",
      "Entropy loss: -0.013615608215332031\n",
      "Loss: -0.382170706987381\n",
      "Actor loss: -1.167763590812683\n",
      "Critic loss: 0.7379628419876099\n",
      "Entropy loss: -0.014122008346021175\n",
      "Loss: -0.443922758102417\n",
      "Actor loss: -1.9312031269073486\n",
      "Critic loss: 1.2807461023330688\n",
      "Entropy loss: -0.014091745018959045\n",
      "Loss: -0.6645487546920776\n",
      "Actor loss: -0.7132646441459656\n",
      "Critic loss: 0.3528386652469635\n",
      "Entropy loss: -0.013717311434447765\n",
      "Loss: -0.3741433024406433\n",
      "Actor loss: -0.026240473613142967\n",
      "Critic loss: 0.0005866882274858654\n",
      "Entropy loss: -0.013458739966154099\n",
      "Loss: -0.03911252319812775\n",
      "Actor loss: -0.8790335655212402\n",
      "Critic loss: 0.4745255410671234\n",
      "Entropy loss: -0.014572568237781525\n",
      "Loss: -0.41908058524131775\n",
      "Actor loss: -0.655539333820343\n",
      "Critic loss: 0.30281302332878113\n",
      "Entropy loss: -0.014218985103070736\n",
      "Loss: -0.3669452965259552\n",
      "Actor loss: -0.5470361113548279\n",
      "Critic loss: 0.10150349140167236\n",
      "Entropy loss: -0.013689032755792141\n",
      "Loss: -0.45922166109085083\n",
      "Actor loss: -0.9311582446098328\n",
      "Critic loss: 0.2379634976387024\n",
      "Entropy loss: -0.013426028192043304\n",
      "Loss: -0.7066207528114319\n",
      "Actor loss: -0.4935780167579651\n",
      "Critic loss: 0.25651630759239197\n",
      "Entropy loss: -0.013325592502951622\n",
      "Loss: -0.2503873109817505\n",
      "Actor loss: -2.658130168914795\n",
      "Critic loss: 2.26314115524292\n",
      "Entropy loss: -0.013545847497880459\n",
      "Loss: -0.40853485465049744\n",
      "Actor loss: -1.9189748764038086\n",
      "Critic loss: 1.0988099575042725\n",
      "Entropy loss: -0.015249215066432953\n",
      "Loss: -0.8354141116142273\n",
      "Actor loss: -0.7406918406486511\n",
      "Critic loss: 0.444220632314682\n",
      "Entropy loss: -0.013720199465751648\n",
      "Loss: -0.31019139289855957\n",
      "Actor loss: -0.7100433111190796\n",
      "Critic loss: 0.09324578195810318\n",
      "Entropy loss: -0.011895645409822464\n",
      "Loss: -0.628693163394928\n",
      "Actor loss: 0.027868065983057022\n",
      "Critic loss: 0.0003659260692074895\n",
      "Entropy loss: -0.01510884240269661\n",
      "Loss: 0.01312514953315258\n",
      "Actor loss: -0.5063410997390747\n",
      "Critic loss: 0.1780620813369751\n",
      "Entropy loss: -0.013203348033130169\n",
      "Loss: -0.34148237109184265\n",
      "Actor loss: -1.896201491355896\n",
      "Critic loss: 0.6754723787307739\n",
      "Entropy loss: -0.011989159509539604\n",
      "Loss: -1.2327182292938232\n",
      "Actor loss: -2.8640217781066895\n",
      "Critic loss: 2.6504807472229004\n",
      "Entropy loss: -0.01575717143714428\n",
      "Loss: -0.2292982041835785\n",
      "Actor loss: -2.510328769683838\n",
      "Critic loss: 2.718883991241455\n",
      "Entropy loss: -0.013438081368803978\n",
      "Loss: 0.19511714577674866\n",
      "Actor loss: -2.5701956748962402\n",
      "Critic loss: 2.269202947616577\n",
      "Entropy loss: -0.011654835194349289\n",
      "Loss: -0.3126475512981415\n",
      "Actor loss: -0.5750210881233215\n",
      "Critic loss: 0.2628726065158844\n",
      "Entropy loss: -0.01285117119550705\n",
      "Loss: -0.3249996602535248\n",
      "Actor loss: -0.7847873568534851\n",
      "Critic loss: 0.2829349637031555\n",
      "Entropy loss: -0.013756981119513512\n",
      "Loss: -0.5156093835830688\n",
      "Actor loss: -0.005355787929147482\n",
      "Critic loss: 2.3865000912337564e-05\n",
      "Entropy loss: -0.013664303347468376\n",
      "Loss: -0.01899622566998005\n",
      "Actor loss: -1.3517426252365112\n",
      "Critic loss: 1.4309217929840088\n",
      "Entropy loss: -0.013280827552080154\n",
      "Loss: 0.0658983439207077\n",
      "Actor loss: -2.135769844055176\n",
      "Critic loss: 1.50043523311615\n",
      "Entropy loss: -0.014666629955172539\n",
      "Loss: -0.6500012278556824\n",
      "Actor loss: -3.8141112327575684\n",
      "Critic loss: 4.2170186042785645\n",
      "Entropy loss: -0.013946957886219025\n",
      "Loss: 0.38896042108535767\n",
      "Actor loss: -1.407158374786377\n",
      "Critic loss: 0.8555543422698975\n",
      "Entropy loss: -0.013426436111330986\n",
      "Loss: -0.5650304555892944\n",
      "Actor loss: -2.8410158157348633\n",
      "Critic loss: 2.5190908908843994\n",
      "Entropy loss: -0.013319510035216808\n",
      "Loss: -0.33524444699287415\n",
      "Actor loss: -0.5384117364883423\n",
      "Critic loss: 0.08805596828460693\n",
      "Entropy loss: -0.012793666683137417\n",
      "Loss: -0.46314942836761475\n",
      "Actor loss: 0.26764750480651855\n",
      "Critic loss: 0.02794249728322029\n",
      "Entropy loss: -0.015652375295758247\n",
      "Loss: 0.27993762493133545\n",
      "Actor loss: -1.5433143377304077\n",
      "Critic loss: 0.6219934225082397\n",
      "Entropy loss: -0.012900194153189659\n",
      "Loss: -0.934221088886261\n",
      "Actor loss: -2.187040090560913\n",
      "Critic loss: 2.740262746810913\n",
      "Entropy loss: -0.014188535511493683\n",
      "Loss: 0.5390341281890869\n",
      "Actor loss: -2.904542922973633\n",
      "Critic loss: 3.8387861251831055\n",
      "Entropy loss: -0.013772300444543362\n",
      "Loss: 0.9204708933830261\n",
      "Actor loss: -4.231051445007324\n",
      "Critic loss: 2.6969869136810303\n",
      "Entropy loss: -0.010958833619952202\n",
      "Loss: -1.5450233221054077\n",
      "Actor loss: -2.543860912322998\n",
      "Critic loss: 2.2131845951080322\n",
      "Entropy loss: -0.014539828523993492\n",
      "Loss: -0.34521615505218506\n",
      "Actor loss: -1.726901888847351\n",
      "Critic loss: 1.5370486974716187\n",
      "Entropy loss: -0.01484561525285244\n",
      "Loss: -0.20469880104064941\n",
      "Actor loss: -2.2760729789733887\n",
      "Critic loss: 1.9018614292144775\n",
      "Entropy loss: -0.012150525115430355\n",
      "Loss: -0.38636207580566406\n",
      "Actor loss: -3.661919116973877\n",
      "Critic loss: 2.1379635334014893\n",
      "Entropy loss: -0.012053023092448711\n",
      "Loss: -1.536008596420288\n",
      "Actor loss: -1.603341817855835\n",
      "Critic loss: 1.7689030170440674\n",
      "Entropy loss: -0.012982271611690521\n",
      "Loss: 0.1525789201259613\n",
      "Actor loss: -3.736785411834717\n",
      "Critic loss: 3.109689235687256\n",
      "Entropy loss: -0.012062824331223965\n",
      "Loss: -0.6391590237617493\n",
      "Actor loss: -4.5891265869140625\n",
      "Critic loss: 3.8064496517181396\n",
      "Entropy loss: -0.012635896913707256\n",
      "Loss: -0.7953128218650818\n",
      "Actor loss: -0.6286773085594177\n",
      "Critic loss: 0.29568108916282654\n",
      "Entropy loss: -0.012107424437999725\n",
      "Loss: -0.3451036512851715\n",
      "Actor loss: -2.028080463409424\n",
      "Critic loss: 1.1244874000549316\n",
      "Entropy loss: -0.01364787295460701\n",
      "Loss: -0.9172409176826477\n",
      "Actor loss: -5.63847541809082\n",
      "Critic loss: 6.284476280212402\n",
      "Entropy loss: -0.011316611431539059\n",
      "Loss: 0.6346842646598816\n",
      "Actor loss: -5.2145891189575195\n",
      "Critic loss: 4.196025371551514\n",
      "Entropy loss: -0.013361316174268723\n",
      "Loss: -1.031925082206726\n",
      "Actor loss: -56.85297775268555\n",
      "Critic loss: 4.763362884521484\n",
      "Entropy loss: -0.008659131824970245\n",
      "Loss: -52.09827423095703\n",
      "Actor loss: -0.4810257852077484\n",
      "Critic loss: 0.22031553089618683\n",
      "Entropy loss: -0.013186059892177582\n",
      "Loss: -0.27389630675315857\n",
      "Actor loss: -4.479450702667236\n",
      "Critic loss: 0.6756391525268555\n",
      "Entropy loss: -0.011827386915683746\n",
      "Loss: -3.815639019012451\n",
      "Actor loss: -2.5355606079101562\n",
      "Critic loss: 2.0189595222473145\n",
      "Entropy loss: -0.010131888091564178\n",
      "Loss: -0.5267329812049866\n",
      "Actor loss: -0.2641506791114807\n",
      "Critic loss: 0.14477644860744476\n",
      "Entropy loss: -0.008969629183411598\n",
      "Loss: -0.128343865275383\n",
      "Actor loss: 0.044954054057598114\n",
      "Critic loss: 0.001152254524640739\n",
      "Entropy loss: -0.009548734873533249\n",
      "Loss: 0.036557573825120926\n",
      "Actor loss: -31.45125389099121\n",
      "Critic loss: 3.268958806991577\n",
      "Entropy loss: -0.009078450500965118\n",
      "Loss: -28.191373825073242\n",
      "Actor loss: -5.934867858886719\n",
      "Critic loss: 0.6862805485725403\n",
      "Entropy loss: -0.009178213775157928\n",
      "Loss: -5.257765293121338\n",
      "Actor loss: 0.9946742057800293\n",
      "Critic loss: 0.4117825925350189\n",
      "Entropy loss: -0.010319050401449203\n",
      "Loss: 1.3961378335952759\n",
      "Actor loss: -1168.6365966796875\n",
      "Critic loss: 14.763567924499512\n",
      "Entropy loss: 0.001707001356408\n",
      "Loss: -1153.871337890625\n",
      "Actor loss: -8.228681564331055\n",
      "Critic loss: 1.9552674293518066\n",
      "Entropy loss: -0.011321565136313438\n",
      "Loss: -6.284735679626465\n",
      "Actor loss: 17.786540985107422\n",
      "Critic loss: 1.0740420818328857\n",
      "Entropy loss: -0.005221243016421795\n",
      "Loss: 18.855361938476562\n",
      "Actor loss: -2215.43408203125\n",
      "Critic loss: 23.525787353515625\n",
      "Entropy loss: 0.007660452276468277\n",
      "Loss: -2191.900634765625\n",
      "Actor loss: 104.36521911621094\n",
      "Critic loss: 0.5866082906723022\n",
      "Entropy loss: -0.0035667321644723415\n",
      "Loss: 104.9482650756836\n",
      "Actor loss: -26.66718864440918\n",
      "Critic loss: 1.2235016822814941\n",
      "Entropy loss: -0.003737193299457431\n",
      "Loss: -25.447423934936523\n",
      "Actor loss: -984.7066650390625\n",
      "Critic loss: 12.134742736816406\n",
      "Entropy loss: -0.003488322254270315\n",
      "Loss: -972.5753784179688\n",
      "Actor loss: 76.16661834716797\n",
      "Critic loss: 6.262678146362305\n",
      "Entropy loss: -0.008721943013370037\n",
      "Loss: 82.42057800292969\n",
      "Actor loss: -18.813255310058594\n",
      "Critic loss: 0.17957188189029694\n",
      "Entropy loss: -0.0010215628426522017\n",
      "Loss: -18.634706497192383\n",
      "Actor loss: -149.60455322265625\n",
      "Critic loss: 0.7709556221961975\n",
      "Entropy loss: -0.0006716059288010001\n",
      "Loss: -148.8342742919922\n",
      "Actor loss: -8.599433898925781\n",
      "Critic loss: 0.2931441068649292\n",
      "Entropy loss: 0.0017280911561101675\n",
      "Loss: -8.304561614990234\n",
      "Actor loss: -743.0855712890625\n",
      "Critic loss: 28.699705123901367\n",
      "Entropy loss: 0.004041258245706558\n",
      "Loss: -714.3818359375\n",
      "Actor loss: 13.616544723510742\n",
      "Critic loss: 1.6760669946670532\n",
      "Entropy loss: -0.007806648965924978\n",
      "Loss: 15.284805297851562\n",
      "Actor loss: -1213.4727783203125\n",
      "Critic loss: 7.404110431671143\n",
      "Entropy loss: 0.001359323039650917\n",
      "Loss: -1206.0673828125\n",
      "Actor loss: 9.832723617553711\n",
      "Critic loss: 28.729156494140625\n",
      "Entropy loss: -0.00887894444167614\n",
      "Loss: 38.553001403808594\n",
      "Actor loss: -662.3221435546875\n",
      "Critic loss: 20.903066635131836\n",
      "Entropy loss: 0.006247218698263168\n",
      "Loss: -641.412841796875\n",
      "Actor loss: -716.330322265625\n",
      "Critic loss: 58.8848876953125\n",
      "Entropy loss: -0.0027745524421334267\n",
      "Loss: -657.4481811523438\n",
      "Actor loss: 459.5986022949219\n",
      "Critic loss: 139.223388671875\n",
      "Entropy loss: -0.007508492097258568\n",
      "Loss: 598.8145141601562\n",
      "Actor loss: -731.5636596679688\n",
      "Critic loss: 3.1848764419555664\n",
      "Entropy loss: 0.013143269345164299\n",
      "Loss: -728.3656616210938\n",
      "Actor loss: -672.6597290039062\n",
      "Critic loss: 3.0976579189300537\n",
      "Entropy loss: 0.013568373396992683\n",
      "Loss: -669.5485229492188\n",
      "Actor loss: -5375.4912109375\n",
      "Critic loss: 128.55191040039062\n",
      "Entropy loss: 0.013381621800363064\n",
      "Loss: -5246.92626953125\n",
      "Actor loss: 24.89918327331543\n",
      "Critic loss: 284.0528259277344\n",
      "Entropy loss: -0.013036239892244339\n",
      "Loss: 308.93896484375\n",
      "Actor loss: -819.6591796875\n",
      "Critic loss: 50.62224578857422\n",
      "Entropy loss: 0.02652149274945259\n",
      "Loss: -769.0103759765625\n",
      "Actor loss: -2887.572265625\n",
      "Critic loss: 83.50991821289062\n",
      "Entropy loss: 0.0028251647017896175\n",
      "Loss: -2804.059326171875\n",
      "Actor loss: 31.9865779876709\n",
      "Critic loss: 18.083415985107422\n",
      "Entropy loss: -0.008696367964148521\n",
      "Loss: 50.06129455566406\n",
      "Actor loss: 122.05442810058594\n",
      "Critic loss: 433.0852966308594\n",
      "Entropy loss: -0.005865040235221386\n",
      "Loss: 555.1338500976562\n",
      "Actor loss: -1935.447265625\n",
      "Critic loss: 84.47484588623047\n",
      "Entropy loss: 0.06114447861909866\n",
      "Loss: -1850.9112548828125\n",
      "Actor loss: -7513.197265625\n",
      "Critic loss: 610.5152587890625\n",
      "Entropy loss: 0.03486054018139839\n",
      "Loss: -6902.6474609375\n",
      "Actor loss: 34.0410270690918\n",
      "Critic loss: 353.3141174316406\n",
      "Entropy loss: -0.009714270010590553\n",
      "Loss: 387.3454284667969\n",
      "Actor loss: 1319.199951171875\n",
      "Critic loss: 3.8011045455932617\n",
      "Entropy loss: 0.018567025661468506\n",
      "Loss: 1323.0196533203125\n",
      "Actor loss: -8121.95458984375\n",
      "Critic loss: 254.14971923828125\n",
      "Entropy loss: 0.030957769602537155\n",
      "Loss: -7867.77392578125\n",
      "Actor loss: 818.89501953125\n",
      "Critic loss: 92.18624877929688\n",
      "Entropy loss: -0.00017424486577510834\n",
      "Loss: 911.0811157226562\n",
      "Actor loss: 523.2260131835938\n",
      "Critic loss: 1.7683950662612915\n",
      "Entropy loss: 0.007410552352666855\n",
      "Loss: 525.0017700195312\n",
      "Actor loss: 6259.31787109375\n",
      "Critic loss: 253.694091796875\n",
      "Entropy loss: 0.02993394434452057\n",
      "Loss: 6513.04150390625\n",
      "Actor loss: 207.9676971435547\n",
      "Critic loss: 3.7451603412628174\n",
      "Entropy loss: 0.054027676582336426\n",
      "Loss: 211.7668914794922\n",
      "Actor loss: -4202.29296875\n",
      "Critic loss: 333.969970703125\n",
      "Entropy loss: 0.058737706393003464\n",
      "Loss: -3868.26416015625\n",
      "Actor loss: -2336.41015625\n",
      "Critic loss: 189.84878540039062\n",
      "Entropy loss: 0.011830414645373821\n",
      "Loss: -2146.549560546875\n",
      "Actor loss: 10.83144760131836\n",
      "Critic loss: 32.198974609375\n",
      "Entropy loss: -0.010449765250086784\n",
      "Loss: 43.01997375488281\n",
      "Actor loss: 390.1529235839844\n",
      "Critic loss: 13.536033630371094\n",
      "Entropy loss: -0.0009001953294500709\n",
      "Loss: 403.6880798339844\n",
      "Actor loss: 753.48388671875\n",
      "Critic loss: 60.06308364868164\n",
      "Entropy loss: 0.0018954932456836104\n",
      "Loss: 813.5488891601562\n",
      "Actor loss: 4263.0078125\n",
      "Critic loss: 148.9247283935547\n",
      "Entropy loss: 0.020301807671785355\n",
      "Loss: 4411.953125\n",
      "Actor loss: -4235.3505859375\n",
      "Critic loss: 112.0068359375\n",
      "Entropy loss: 0.05140285938978195\n",
      "Loss: -4123.29248046875\n",
      "Actor loss: -5312.59033203125\n",
      "Critic loss: 193.1237335205078\n",
      "Entropy loss: 0.028612252324819565\n",
      "Loss: -5119.43798828125\n",
      "Actor loss: 3.0608632564544678\n",
      "Critic loss: 6.678591728210449\n",
      "Entropy loss: -0.012395083904266357\n",
      "Loss: 9.727060317993164\n",
      "Actor loss: -83.07249450683594\n",
      "Critic loss: 29.28753089904785\n",
      "Entropy loss: -0.004780716262757778\n",
      "Loss: -53.78974533081055\n",
      "Actor loss: 199.90879821777344\n",
      "Critic loss: 114.8917236328125\n",
      "Entropy loss: -0.007813659496605396\n",
      "Loss: 314.792724609375\n",
      "Actor loss: 9018.634765625\n",
      "Critic loss: 1180.1923828125\n",
      "Entropy loss: 0.01365419290959835\n",
      "Loss: 10198.8408203125\n",
      "Actor loss: -31479.62890625\n",
      "Critic loss: 2531.123046875\n",
      "Entropy loss: 0.09832838177680969\n",
      "Loss: -28948.408203125\n",
      "Actor loss: 60.27802658081055\n",
      "Critic loss: 1138.3851318359375\n",
      "Entropy loss: -0.016205094754695892\n",
      "Loss: 1198.64697265625\n",
      "Actor loss: -4859.19091796875\n",
      "Critic loss: 78.1900405883789\n",
      "Entropy loss: 0.04807548224925995\n",
      "Loss: -4780.953125\n",
      "Actor loss: -13757.5\n",
      "Critic loss: 670.6107177734375\n",
      "Entropy loss: 0.03103751689195633\n",
      "Loss: -13086.8583984375\n",
      "Actor loss: 184.23590087890625\n",
      "Critic loss: 448.64532470703125\n",
      "Entropy loss: -0.012698141857981682\n",
      "Loss: 632.8685302734375\n",
      "Actor loss: -13663.35546875\n",
      "Critic loss: 367.488037109375\n",
      "Entropy loss: 0.011770021170377731\n",
      "Loss: -13295.85546875\n",
      "Actor loss: 54.63166427612305\n",
      "Critic loss: 1355.639892578125\n",
      "Entropy loss: -0.01756027713418007\n",
      "Loss: 1410.2540283203125\n",
      "Actor loss: -5496.67529296875\n",
      "Critic loss: 232.701416015625\n",
      "Entropy loss: 0.056731484830379486\n",
      "Loss: -5263.9169921875\n",
      "Actor loss: 2647.46044921875\n",
      "Critic loss: 141.78363037109375\n",
      "Entropy loss: 0.019114021211862564\n",
      "Loss: 2789.26318359375\n",
      "Actor loss: -11862.404296875\n",
      "Critic loss: 2116.16357421875\n",
      "Entropy loss: 0.04584489390254021\n",
      "Loss: -9746.1943359375\n",
      "Actor loss: 7.658862113952637\n",
      "Critic loss: 20.804399490356445\n",
      "Entropy loss: -0.02090732753276825\n",
      "Loss: 28.442354202270508\n",
      "Actor loss: 63.08098602294922\n",
      "Critic loss: 1419.585205078125\n",
      "Entropy loss: -0.020239487290382385\n",
      "Loss: 1482.6458740234375\n",
      "Actor loss: 6701.8095703125\n",
      "Critic loss: 140.05271911621094\n",
      "Entropy loss: 0.027544721961021423\n",
      "Loss: 6841.8896484375\n",
      "Actor loss: -6578.07421875\n",
      "Critic loss: 788.4592895507812\n",
      "Entropy loss: 0.06229778379201889\n",
      "Loss: -5789.55224609375\n",
      "Actor loss: -2095.116943359375\n",
      "Critic loss: 68.05513763427734\n",
      "Entropy loss: 0.0016084186499938369\n",
      "Loss: -2027.0601806640625\n",
      "Actor loss: 583.6492309570312\n",
      "Critic loss: 252.63400268554688\n",
      "Entropy loss: -0.008314833045005798\n",
      "Loss: 836.27490234375\n",
      "Actor loss: 3372.919677734375\n",
      "Critic loss: 1108.949951171875\n",
      "Entropy loss: 0.02054673247039318\n",
      "Loss: 4481.89013671875\n",
      "Actor loss: -9928.9560546875\n",
      "Critic loss: 541.7764892578125\n",
      "Entropy loss: 0.0867968499660492\n",
      "Loss: -9387.0927734375\n",
      "Actor loss: -30678.771484375\n",
      "Critic loss: 2701.19970703125\n",
      "Entropy loss: 0.04193422198295593\n",
      "Loss: -27977.53125\n",
      "Actor loss: 120.93583679199219\n",
      "Critic loss: 3132.59912109375\n",
      "Entropy loss: -0.022066593170166016\n",
      "Loss: 3253.512939453125\n",
      "Actor loss: -5451.95849609375\n",
      "Critic loss: 208.66390991210938\n",
      "Entropy loss: 0.04074706509709358\n",
      "Loss: -5243.25390625\n",
      "Actor loss: -126.65470886230469\n",
      "Critic loss: 0.20561103522777557\n",
      "Entropy loss: 0.02040693163871765\n",
      "Loss: -126.4286880493164\n",
      "Actor loss: -7144.609375\n",
      "Critic loss: 460.393798828125\n",
      "Entropy loss: 0.01748092845082283\n",
      "Loss: -6684.1982421875\n",
      "Actor loss: -18.99470329284668\n",
      "Critic loss: 144.5072784423828\n",
      "Entropy loss: -0.01778559759259224\n",
      "Loss: 125.49478912353516\n",
      "Actor loss: 149.90016174316406\n",
      "Critic loss: 5832.55322265625\n",
      "Entropy loss: -0.021159013733267784\n",
      "Loss: 5982.4326171875\n",
      "Actor loss: -6092.67333984375\n",
      "Critic loss: 101.94918060302734\n",
      "Entropy loss: 0.10805729031562805\n",
      "Loss: -5990.6162109375\n",
      "Actor loss: -38170.99609375\n",
      "Critic loss: 5084.2734375\n",
      "Entropy loss: 0.10002194344997406\n",
      "Loss: -33086.62109375\n",
      "Actor loss: 14.847137451171875\n",
      "Critic loss: 74.82962036132812\n",
      "Entropy loss: -0.021760722622275352\n",
      "Loss: 89.65499877929688\n",
      "Actor loss: 1766.815185546875\n",
      "Critic loss: 1639.0009765625\n",
      "Entropy loss: -0.009912865236401558\n",
      "Loss: 3405.80615234375\n",
      "Actor loss: -18588.07421875\n",
      "Critic loss: 1014.9102783203125\n",
      "Entropy loss: 0.052221521735191345\n",
      "Loss: -17573.111328125\n",
      "Actor loss: -2794.783935546875\n",
      "Critic loss: 1171.4351806640625\n",
      "Entropy loss: -0.004971164744347334\n",
      "Loss: -1623.353759765625\n",
      "Actor loss: 68.6070556640625\n",
      "Critic loss: 1090.4305419921875\n",
      "Entropy loss: -0.024560291320085526\n",
      "Loss: 1159.0130615234375\n",
      "Actor loss: 174.01670837402344\n",
      "Critic loss: 1215.901611328125\n",
      "Entropy loss: -0.010581212118268013\n",
      "Loss: 1389.90771484375\n",
      "Actor loss: -425.60302734375\n",
      "Critic loss: 0.8062100410461426\n",
      "Entropy loss: 0.046539269387722015\n",
      "Loss: -424.7502746582031\n",
      "Actor loss: -2811.76220703125\n",
      "Critic loss: 64.92761993408203\n",
      "Entropy loss: 0.05088314041495323\n",
      "Loss: -2746.78369140625\n",
      "Actor loss: -11435.7490234375\n",
      "Critic loss: 2289.774169921875\n",
      "Entropy loss: 0.03868589922785759\n",
      "Loss: -9145.935546875\n",
      "Actor loss: 13.203145980834961\n",
      "Critic loss: 50.5606803894043\n",
      "Entropy loss: -0.022494181990623474\n",
      "Loss: 63.741329193115234\n",
      "Actor loss: 1.619429111480713\n",
      "Critic loss: 1.001196265220642\n",
      "Entropy loss: -0.02007996290922165\n",
      "Loss: 2.600545644760132\n",
      "Actor loss: 99.39656066894531\n",
      "Critic loss: 3692.909423828125\n",
      "Entropy loss: -0.02032175287604332\n",
      "Loss: 3792.28564453125\n",
      "Actor loss: -9589.27734375\n",
      "Critic loss: 699.5343627929688\n",
      "Entropy loss: 0.07813291251659393\n",
      "Loss: -8889.6650390625\n",
      "Actor loss: -8015.54638671875\n",
      "Critic loss: 610.0645751953125\n",
      "Entropy loss: 0.03882183879613876\n",
      "Loss: -7405.44287109375\n",
      "Actor loss: 43.578094482421875\n",
      "Critic loss: 1589.260986328125\n",
      "Entropy loss: -0.014958987943828106\n",
      "Loss: 1632.8240966796875\n",
      "Actor loss: -14256.83203125\n",
      "Critic loss: 1124.0052490234375\n",
      "Entropy loss: 0.05557810515165329\n",
      "Loss: -13132.771484375\n",
      "Actor loss: -4689.3056640625\n",
      "Critic loss: 900.7748413085938\n",
      "Entropy loss: 0.003395531792193651\n",
      "Loss: -3788.52734375\n",
      "Actor loss: 107.010009765625\n",
      "Critic loss: 3257.123291015625\n",
      "Entropy loss: -0.022950246930122375\n",
      "Loss: 3364.1103515625\n",
      "Actor loss: 2842.77490234375\n",
      "Critic loss: 79.45115661621094\n",
      "Entropy loss: 0.03915882110595703\n",
      "Loss: 2922.26513671875\n",
      "Actor loss: -30693.513671875\n",
      "Critic loss: 2798.223388671875\n",
      "Entropy loss: 0.0725250095129013\n",
      "Loss: -27895.21875\n",
      "Actor loss: 128.49317932128906\n",
      "Critic loss: 2567.4482421875\n",
      "Entropy loss: -0.012984438799321651\n",
      "Loss: 2695.928466796875\n",
      "Actor loss: -26156.31640625\n",
      "Critic loss: 6621.49951171875\n",
      "Entropy loss: 0.05882009491324425\n",
      "Loss: -19534.7578125\n",
      "Actor loss: 56.051231384277344\n",
      "Critic loss: 847.9732055664062\n",
      "Entropy loss: -0.023700786754488945\n",
      "Loss: 904.000732421875\n",
      "Actor loss: 125.6930160522461\n",
      "Critic loss: 9469.2568359375\n",
      "Entropy loss: -0.016821425408124924\n",
      "Loss: 9594.93359375\n",
      "Actor loss: -41616.203125\n",
      "Critic loss: 7997.37841796875\n",
      "Entropy loss: 0.14695581793785095\n",
      "Loss: -33618.67578125\n",
      "Actor loss: -236.91830444335938\n",
      "Critic loss: 1454.190185546875\n",
      "Entropy loss: -0.011705669574439526\n",
      "Loss: 1217.2601318359375\n",
      "Actor loss: 75.3392562866211\n",
      "Critic loss: 1320.2227783203125\n",
      "Entropy loss: -0.023385917767882347\n",
      "Loss: 1395.53857421875\n",
      "Actor loss: 431.4656982421875\n",
      "Critic loss: 16657.255859375\n",
      "Entropy loss: -0.01144935842603445\n",
      "Loss: 17088.708984375\n",
      "Actor loss: -113929.3359375\n",
      "Critic loss: 23900.4453125\n",
      "Entropy loss: 0.21996906399726868\n",
      "Loss: -90028.671875\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n",
      "Actor loss: nan\n",
      "Critic loss: nan\n",
      "Entropy loss: nan\n",
      "Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\halfcheetach.a2c.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nkvch/studia/USD/halfcheetach.a2c.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nkvch/studia/USD/halfcheetach.a2c.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     steps \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mrun_episode()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nkvch/studia/USD/halfcheetach.a2c.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpisode: \u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m}\u001b[39;00m\u001b[39m, steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\halfcheetach.a2c.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nkvch/studia/USD/halfcheetach.a2c.ipynb#X16sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nkvch/studia/USD/halfcheetach.a2c.ipynb#X16sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39m# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nkvch/studia/USD/halfcheetach.a2c.ipynb#X16sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nkvch/studia/USD/halfcheetach.a2c.ipynb#X16sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nkvch/studia/USD/halfcheetach.a2c.ipynb#X16sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisodes_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:496\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    493\u001b[0m device_params \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mview_as_real(x) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m device_params]\n\u001b[0;32m    495\u001b[0m \u001b[39m# update steps\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m torch\u001b[39m.\u001b[39;49m_foreach_add_(device_state_steps, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    498\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    499\u001b[0m     \u001b[39m# Re-use the intermediate memory (device_grads) already allocated for maximize\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[39mif\u001b[39;00m maximize:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    steps = agent.run_episode()\n",
    "    print(f'Episode: {episode}, steps: {steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
