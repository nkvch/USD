{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.env_id = \"HalfCheetah-v4\"\n",
    "args.total_timesteps = 10_000_000\n",
    "args.num_envs = 16\n",
    "args.num_steps = 5\n",
    "args.learning_rate = 5e-4\n",
    "args.actor_layers = [64, 64]\n",
    "args.critic_layers  = [64, 64]\n",
    "args.gamma = 0.99\n",
    "args.gae = 1.0\n",
    "args.value_coef = 0.5\n",
    "args.entropy_coef = 0.01\n",
    "args.clip_grad_norm = 0.5\n",
    "args.seed = 0\n",
    "\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.num_updates = int(args.total_timesteps // args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, capture_video=False, run_dir=\".\"):\n",
    "    def thunk():\n",
    "        if capture_video:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            # env = gym.wrappers.RecordVideo(\n",
    "            #     env=env,\n",
    "            #     video_folder=f\"{run_dir}/videos\",\n",
    "            #     episode_trigger=lambda x: x,\n",
    "            #     disable_logger=True,\n",
    "            # )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.FlattenObservation(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda state: np.clip(state, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, flags, values, last_value, args):\n",
    "    advantages = torch.zeros((args.num_steps, args.num_envs))\n",
    "    adv = torch.zeros(args.num_envs)\n",
    "\n",
    "    for i in reversed(range(args.num_steps)):\n",
    "        returns = rewards[i] + args.gamma * flags[i] * last_value\n",
    "        delta = returns - values[i]\n",
    "\n",
    "        adv = delta + args.gamma * args.gae * flags[i] * adv\n",
    "        advantages[i] = adv\n",
    "\n",
    "        last_value = values[i]\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, num_steps, num_envs, observation_shape, action_shape):\n",
    "        self.states = np.zeros((num_steps, num_envs, *observation_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((num_steps, num_envs, *action_shape), dtype=np.float32)\n",
    "        self.rewards = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        self.flags = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        self.values = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "\n",
    "        self.step = 0\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def push(self, state, action, reward, flag, value):\n",
    "        self.states[self.step] = state\n",
    "        self.actions[self.step] = action\n",
    "        self.rewards[self.step] = reward\n",
    "        self.flags[self.step] = flag\n",
    "        self.values[self.step] = value\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def get(self):\n",
    "        return (\n",
    "            torch.from_numpy(self.states),\n",
    "            torch.from_numpy(self.actions),\n",
    "            torch.from_numpy(self.rewards),\n",
    "            torch.from_numpy(self.flags),\n",
    "            torch.from_numpy(self.values),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, observation_shape, action_dim, actor_layers, critic_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor_net = self._build_net(observation_shape, actor_layers)\n",
    "        self.critic_net = self._build_net(observation_shape, critic_layers)\n",
    "\n",
    "        self.actor_net.append(self._build_linear(actor_layers[-1], action_dim, std=0.01))\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n",
    "\n",
    "        self.critic_net.append(self._build_linear(critic_layers[-1], 1, std=1.0))\n",
    "\n",
    "    def _build_linear(self, in_size, out_size, apply_init=True, std=np.sqrt(2), bias_const=0.0):\n",
    "        layer = nn.Linear(in_size, out_size)\n",
    "\n",
    "        if apply_init:\n",
    "            torch.nn.init.orthogonal_(layer.weight, std)\n",
    "            torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    def _build_net(self, observation_shape, hidden_layers):\n",
    "        layers = nn.Sequential()\n",
    "        in_size = np.prod(observation_shape)\n",
    "\n",
    "        for out_size in hidden_layers:\n",
    "            layers.append(self._build_linear(in_size, out_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            in_size = out_size\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = self.actor_net(state)\n",
    "        std = self.actor_logstd.expand_as(mean).exp()\n",
    "        distribution = Normal(mean, std)\n",
    "\n",
    "        action = distribution.sample()\n",
    "\n",
    "        value = self.critic_net(state).squeeze(-1)\n",
    "\n",
    "        return action, value\n",
    "\n",
    "    def evaluate(self, states, actions):\n",
    "        mean = self.actor_net(states)\n",
    "        std = self.actor_logstd.expand_as(mean).exp()\n",
    "        distribution = Normal(mean, std)\n",
    "\n",
    "        log_probs = distribution.log_prob(actions).sum(-1)\n",
    "        entropy = distribution.entropy().sum(-1)\n",
    "\n",
    "        values = self.critic_net(states).squeeze(-1)\n",
    "\n",
    "        return log_probs, values, entropy\n",
    "\n",
    "    def critic(self, state):\n",
    "        return self.critic_net(state).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ninkhivs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>▁█▆▇</td></tr><tr><td>critic_loss</td><td>▁▇▇█</td></tr><tr><td>entropy_loss</td><td>▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>0.04013</td></tr><tr><td>critic_loss</td><td>31.015</td></tr><tr><td>entropy_loss</td><td>8.51363</td></tr><tr><td>mean_return</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-lion-19</strong> at: <a href='https://wandb.ai/enelpe/halfcheetah/runs/ninkhivs' target=\"_blank\">https://wandb.ai/enelpe/halfcheetah/runs/ninkhivs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231210_211818-ninkhivs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ninkhivs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nkvch\\studia\\USD\\wandb\\run-20231210_211928-oscncid2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/enelpe/halfcheetah/runs/oscncid2' target=\"_blank\">lilac-planet-20</a></strong> to <a href='https://wandb.ai/enelpe/halfcheetah' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/enelpe/halfcheetah' target=\"_blank\">https://wandb.ai/enelpe/halfcheetah</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/enelpe/halfcheetah/runs/oscncid2' target=\"_blank\">https://wandb.ai/enelpe/halfcheetah/runs/oscncid2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/enelpe/halfcheetah/runs/oscncid2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x21040f24510>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"halfcheetah\",\n",
    "    config={\n",
    "      \"tutorial\": \"True\",\n",
    "      \"learning_rate\": args.learning_rate,\n",
    "      \"gamma\": args.gamma,\n",
    "      \"gae\": args.gae,\n",
    "      \"value_coef\": args.value_coef,\n",
    "      \"entropy_coef\": args.entropy_coef,\n",
    "      \"clip_grad_norm\": args.clip_grad_norm,\n",
    "      \"seed\": args.seed,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, run_name, run_dir):\n",
    "    # Create tensorboard writer and save hyperparameters\n",
    "    writer = SummaryWriter(run_dir)\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # Create vectorized environment(s)\n",
    "    envs = gym.vector.AsyncVectorEnv([make_env(args.env_id) for _ in range(args.num_envs)])\n",
    "\n",
    "    # Metadata about the environment\n",
    "    observation_shape = envs.single_observation_space.shape\n",
    "    action_shape = envs.single_action_space.shape\n",
    "    action_dim = np.prod(action_shape)\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    if args.seed:\n",
    "        torch.manual_seed(args.seed)\n",
    "        state, _ = envs.reset(seed=args.seed)\n",
    "    else:\n",
    "        state, _ = envs.reset()\n",
    "\n",
    "    # Create policy network and optimizer\n",
    "    policy = ActorCriticNet(observation_shape, action_dim, args.actor_layers, args.critic_layers)\n",
    "    optimizer = optim.RMSprop(policy.parameters(), lr=args.learning_rate, alpha=0.99, eps=1e-5)\n",
    "\n",
    "    # Create buffers\n",
    "    rollout_buffer = RolloutBuffer(args.num_steps, args.num_envs, observation_shape, action_shape)\n",
    "\n",
    "    # Remove unnecessary variables\n",
    "    del action_dim\n",
    "\n",
    "    global_step = 0\n",
    "    log_episodic_returns, log_episodic_lengths = [], []\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    # Main loop\n",
    "    for iter in tqdm(range(args.num_updates)):\n",
    "        for _ in range(args.num_steps):\n",
    "            # Update global step\n",
    "            global_step += 1 * args.num_envs\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get action\n",
    "                action, value = policy(torch.from_numpy(state).float())\n",
    "\n",
    "            # Perform action\n",
    "            action = action.cpu().numpy()\n",
    "            next_state, reward, terminated, truncated, infos = envs.step(action)\n",
    "\n",
    "            # Store transition\n",
    "            flag = 1.0 - np.logical_or(terminated, truncated)\n",
    "            value = value.cpu().numpy()\n",
    "            rollout_buffer.push(state, action, reward, flag, value)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if \"final_info\" not in infos:\n",
    "                continue\n",
    "\n",
    "            # Log episodic return and length\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info is None:\n",
    "                    continue\n",
    "\n",
    "                log_episodic_returns.append(info[\"episode\"][\"r\"])\n",
    "                log_episodic_lengths.append(info[\"episode\"][\"l\"])\n",
    "                writer.add_scalar(\"rollout/episodic_return\", np.mean(log_episodic_returns[-5:]), global_step)\n",
    "                writer.add_scalar(\"rollout/episodic_length\", np.mean(log_episodic_lengths[-5:]), global_step)\n",
    "\n",
    "        # Get transition batch\n",
    "        states, actions, rewards, flags, values = rollout_buffer.get()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_value = policy.critic(torch.from_numpy(next_state).float())\n",
    "\n",
    "        # Calculate advantages and TD target\n",
    "        advantages = compute_advantages(rewards, flags, values, last_value, args)\n",
    "        td_target = advantages + values\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Flatten batch\n",
    "        states = states.reshape(-1, *observation_shape)\n",
    "        actions = actions.reshape(-1, *action_shape)\n",
    "        td_target = td_target.reshape(-1)\n",
    "        advantages = advantages.reshape(-1)\n",
    "\n",
    "        # Compute losses\n",
    "        log_probs, td_predict, entropy = policy.evaluate(states, actions)\n",
    "\n",
    "        actor_loss = (-log_probs * advantages).mean()\n",
    "        critic_loss = mse_loss(td_target, td_predict)\n",
    "        entropy_loss = entropy.mean()\n",
    "\n",
    "        loss = actor_loss + critic_loss * args.value_coef - entropy_loss * args.entropy_coef\n",
    "\n",
    "        wandb.log({\n",
    "            \"actor_loss\": actor_loss,\n",
    "            \"critic_loss\": critic_loss,\n",
    "            \"entropy_loss\": entropy_loss,\n",
    "            \"mean_return\": np.mean(log_episodic_returns[-5:]),\n",
    "        })\n",
    "\n",
    "        # Update policy network\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(policy.parameters(), args.clip_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log training metrics\n",
    "        writer.add_scalar(\"rollout/SPS\", \n",
    "        writer.add_scalar(\"train/loss\", loss, global_step)\n",
    "        writer.add_scalar(\"train/actor_loss\", actor_loss, global_step)\n",
    "        writer.add_scalar(\"train/critic_loss\", critic_loss, global_step)\n",
    "\n",
    "        if iter % 1_000 == 0:\n",
    "            torch.save(policy.state_dict(), f\"{run_dir}/policy.pt\")\n",
    "\n",
    "    # Save final policy\n",
    "    torch.save(policy.state_dict(), f\"{run_dir}/policy.pt\")\n",
    "    print(f\"Saved policy to {run_dir}/policy.pt\")\n",
    "\n",
    "    # Close the environment\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "    # Average of episodic returns (for the last 5% of the training)\n",
    "    indexes = int(len(log_episodic_returns) * 0.05)\n",
    "    mean_train_return = np.mean(log_episodic_returns[-indexes:])\n",
    "    writer.add_scalar(\"rollout/mean_train_return\", mean_train_return, global_step)\n",
    "\n",
    "    return mean_train_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_render(args, run_dir):\n",
    "    # Create environment\n",
    "    env = gym.vector.SyncVectorEnv([make_env(args.env_id, capture_video=True, run_dir=run_dir)])\n",
    "\n",
    "    # Metadata about the environment\n",
    "    observation_shape = env.single_observation_space.shape\n",
    "    action_shape = env.single_action_space.shape\n",
    "    action_dim = np.prod(action_shape)\n",
    "\n",
    "    # Load policy\n",
    "    policy = ActorCriticNet(observation_shape, action_dim, args.actor_layers, args.critic_layers)\n",
    "    filename = f\"{run_dir}/policy.pt\"\n",
    "    print(f\"reading {filename}...\")\n",
    "    policy.load_state_dict(torch.load(filename))\n",
    "    policy.eval()\n",
    "\n",
    "    count_episodes = 0\n",
    "    list_rewards = []\n",
    "\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Run episodes\n",
    "    while count_episodes < 30:\n",
    "        with torch.no_grad():\n",
    "            action, _ = policy(torch.from_numpy(state).float())\n",
    "\n",
    "        action = action.cpu().numpy()\n",
    "        state, _, _, _, infos = env.step(action)\n",
    "\n",
    "        if \"final_info\" in infos:\n",
    "            info = infos[\"final_info\"][0]\n",
    "            returns = info[\"episode\"][\"r\"][0]\n",
    "            count_episodes += 1\n",
    "            list_rewards.append(returns)\n",
    "            print(f\"-> Episode {count_episodes}: {returns} returns\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return np.mean(list_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time: 2023_12_10_21_19_33\n"
     ]
    }
   ],
   "source": [
    "# Create run directory\n",
    "run_time = str(datetime.now()).replace(\" \", \"_\" ).replace(\":\", \"-\").split(\".\")[0].replace(\"-\", \"_\")\n",
    "print(f\"run time: {run_time}\")\n",
    "run_name = \"A2C_PyTorch\"\n",
    "\n",
    "run_dir = Path(f\"runs/{run_name}/{run_time}\")\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(run_dir / \"args.json\", \"w\") as fp:\n",
    "    json.dump(args.__dict__, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing training of A2C_PyTorch on HalfCheetah-v4 for 10000000 timesteps.\n",
      "Results will be saved to: runs\\A2C_PyTorch\\2023_12_10_21_19_33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125000 [00:00<?, ?it/s]c:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|          | 0/125000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommencing training of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39menv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mtotal_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timesteps.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults will be saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m mean_train_return \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining - Mean returns achieved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_train_return\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 114\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, run_name, run_dir)\u001b[0m\n\u001b[0;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Log training metrics\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrollout/SPS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[43mglobal_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m), global_step)\n\u001b[0;32m    115\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, global_step)\n\u001b[0;32m    116\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/actor_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, actor_loss, global_step)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "print(f\"Commencing training of {run_name} on {args.env_id} for {args.total_timesteps} timesteps.\")\n",
    "print(f\"Results will be saved to: {run_dir}\")\n",
    "mean_train_return = train(args=args, run_name=run_name, run_dir=run_dir)\n",
    "print(f\"Training - Mean returns achieved: {mean_train_return}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
