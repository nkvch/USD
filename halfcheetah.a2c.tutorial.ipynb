{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyvirtualdisplay import Display\n",
    "# display = Display(visible=0, size=(800, 600))\n",
    "# display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.env_id = \"HalfCheetah-v4\"\n",
    "args.total_timesteps = 10_000_000\n",
    "args.num_envs = 16\n",
    "args.num_steps = 5\n",
    "args.learning_rate = 5e-4\n",
    "args.actor_layers = [64, 64]\n",
    "args.critic_layers  = [64, 64]\n",
    "args.gamma = 0.99\n",
    "args.gae = 1.0\n",
    "args.value_coef = 0.5\n",
    "args.entropy_coef = 0.01\n",
    "args.clip_grad_norm = 0.5\n",
    "args.seed = 0\n",
    "\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.num_updates = int(args.total_timesteps // args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, capture_video=False, run_dir=\".\"):\n",
    "    def thunk():\n",
    "        if capture_video:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(\n",
    "                env=env,\n",
    "                video_folder=f\"{run_dir}/videos\",\n",
    "                episode_trigger=lambda x: x,\n",
    "                disable_logger=True,\n",
    "            )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.FlattenObservation(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda state: np.clip(state, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, flags, values, last_value, args):\n",
    "    advantages = torch.zeros((args.num_steps, args.num_envs))\n",
    "    adv = torch.zeros(args.num_envs)\n",
    "\n",
    "    for i in reversed(range(args.num_steps)):\n",
    "        returns = rewards[i] + args.gamma * flags[i] * last_value\n",
    "        delta = returns - values[i]\n",
    "\n",
    "        adv = delta + args.gamma * args.gae * flags[i] * adv\n",
    "        advantages[i] = adv\n",
    "\n",
    "        last_value = values[i]\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, num_steps, num_envs, observation_shape, action_shape):\n",
    "        self.states = np.zeros((num_steps, num_envs, *observation_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((num_steps, num_envs, *action_shape), dtype=np.float32)\n",
    "        self.rewards = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        self.flags = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        self.values = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "\n",
    "        self.step = 0\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def push(self, state, action, reward, flag, value):\n",
    "        self.states[self.step] = state\n",
    "        self.actions[self.step] = action\n",
    "        self.rewards[self.step] = reward\n",
    "        self.flags[self.step] = flag\n",
    "        self.values[self.step] = value\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def get(self):\n",
    "        return (\n",
    "            torch.from_numpy(self.states),\n",
    "            torch.from_numpy(self.actions),\n",
    "            torch.from_numpy(self.rewards),\n",
    "            torch.from_numpy(self.flags),\n",
    "            torch.from_numpy(self.values),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, observation_shape, action_dim, actor_layers, critic_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor_net = self._build_net(observation_shape, actor_layers)\n",
    "        self.critic_net = self._build_net(observation_shape, critic_layers)\n",
    "\n",
    "        self.actor_net.append(self._build_linear(actor_layers[-1], action_dim, std=0.01))\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n",
    "\n",
    "        self.critic_net.append(self._build_linear(critic_layers[-1], 1, std=1.0))\n",
    "\n",
    "    def _build_linear(self, in_size, out_size, apply_init=True, std=np.sqrt(2), bias_const=0.0):\n",
    "        layer = nn.Linear(in_size, out_size)\n",
    "\n",
    "        if apply_init:\n",
    "            torch.nn.init.orthogonal_(layer.weight, std)\n",
    "            torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    def _build_net(self, observation_shape, hidden_layers):\n",
    "        layers = nn.Sequential()\n",
    "        in_size = np.prod(observation_shape)\n",
    "\n",
    "        for out_size in hidden_layers:\n",
    "            layers.append(self._build_linear(in_size, out_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            in_size = out_size\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = self.actor_net(state)\n",
    "        print(mean.shape)\n",
    "        std = self.actor_logstd\n",
    "        print(std.shape)\n",
    "        std = std.expand_as(mean).exp()\n",
    "        distribution = Normal(mean, std)\n",
    "\n",
    "        action = distribution.sample()\n",
    "\n",
    "        value = self.critic_net(state).squeeze(-1)\n",
    "\n",
    "        return action, value\n",
    "\n",
    "    def evaluate(self, states, actions):\n",
    "        mean = self.actor_net(states)\n",
    "        std = self.actor_logstd.expand_as(mean).exp()\n",
    "        distribution = Normal(mean, std)\n",
    "\n",
    "        log_probs = distribution.log_prob(actions).sum(-1)\n",
    "        entropy = distribution.entropy().sum(-1)\n",
    "\n",
    "        values = self.critic_net(states).squeeze(-1)\n",
    "\n",
    "        return log_probs, values, entropy\n",
    "\n",
    "    def critic(self, state):\n",
    "        return self.critic_net(state).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, run_name, run_dir):\n",
    "    # Create tensorboard writer and save hyperparameters\n",
    "    writer = SummaryWriter(run_dir)\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # Create vectorized environment(s)\n",
    "    envs = gym.vector.AsyncVectorEnv([make_env(args.env_id) for _ in range(args.num_envs)])\n",
    "\n",
    "    # Metadata about the environment\n",
    "    observation_shape = envs.single_observation_space.shape\n",
    "    action_shape = envs.single_action_space.shape\n",
    "    action_dim = np.prod(action_shape)\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    if args.seed:\n",
    "        torch.manual_seed(args.seed)\n",
    "        state, _ = envs.reset(seed=args.seed)\n",
    "    else:\n",
    "        state, _ = envs.reset()\n",
    "\n",
    "    # Create policy network and optimizer\n",
    "    policy = ActorCriticNet(observation_shape, action_dim, args.actor_layers, args.critic_layers)\n",
    "    optimizer = optim.RMSprop(policy.parameters(), lr=args.learning_rate, alpha=0.99, eps=1e-5)\n",
    "\n",
    "    # Create buffers\n",
    "    rollout_buffer = RolloutBuffer(args.num_steps, args.num_envs, observation_shape, action_shape)\n",
    "\n",
    "    # Remove unnecessary variables\n",
    "    del action_dim\n",
    "\n",
    "    global_step = 0\n",
    "    log_episodic_returns, log_episodic_lengths = [], []\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    # Main loop\n",
    "    for iter in tqdm(range(args.num_updates)):\n",
    "        for _ in range(args.num_steps):\n",
    "            # Update global step\n",
    "            global_step += 1 * args.num_envs\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get action\n",
    "                action, value = policy(torch.from_numpy(state).float())\n",
    "\n",
    "            # Perform action\n",
    "            action = action.cpu().numpy()\n",
    "            next_state, reward, terminated, truncated, infos = envs.step(action)\n",
    "\n",
    "            # Store transition\n",
    "            flag = 1.0 - np.logical_or(terminated, truncated)\n",
    "            value = value.cpu().numpy()\n",
    "            rollout_buffer.push(state, action, reward, flag, value)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if \"final_info\" not in infos:\n",
    "                continue\n",
    "\n",
    "            # Log episodic return and length\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info is None:\n",
    "                    continue\n",
    "\n",
    "                log_episodic_returns.append(info[\"episode\"][\"r\"])\n",
    "                log_episodic_lengths.append(info[\"episode\"][\"l\"])\n",
    "                writer.add_scalar(\"rollout/episodic_return\", np.mean(log_episodic_returns[-5:]), global_step)\n",
    "                writer.add_scalar(\"rollout/episodic_length\", np.mean(log_episodic_lengths[-5:]), global_step)\n",
    "\n",
    "        # Get transition batch\n",
    "        states, actions, rewards, flags, values = rollout_buffer.get()\n",
    "\n",
    "        print(states.shape, actions.shape, rewards.shape, flags.shape, values.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_value = policy.critic(torch.from_numpy(next_state).float())\n",
    "\n",
    "        # Calculate advantages and TD target\n",
    "        advantages = compute_advantages(rewards, flags, values, last_value, args)\n",
    "        td_target = advantages + values\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Flatten batch\n",
    "        states = states.reshape(-1, *observation_shape)\n",
    "        actions = actions.reshape(-1, *action_shape)\n",
    "        td_target = td_target.reshape(-1)\n",
    "        advantages = advantages.reshape(-1)\n",
    "\n",
    "        # Compute losses\n",
    "        log_probs, td_predict, entropy = policy.evaluate(states, actions)\n",
    "\n",
    "        print(log_probs, td_predict, entropy)\n",
    "\n",
    "        actor_loss = (-log_probs * advantages).mean()\n",
    "        critic_loss = mse_loss(td_target, td_predict)\n",
    "        entropy_loss = entropy.mean()\n",
    "\n",
    "        loss = actor_loss + critic_loss * args.value_coef - entropy_loss * args.entropy_coef\n",
    "\n",
    "        print(loss)\n",
    "\n",
    "        # Update policy network\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(policy.parameters(), args.clip_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log training metrics\n",
    "        writer.add_scalar(\"rollout/SPS\", int(global_step / (time.process_time() - start_time)), global_step)\n",
    "        writer.add_scalar(\"train/loss\", loss, global_step)\n",
    "        writer.add_scalar(\"train/actor_loss\", actor_loss, global_step)\n",
    "        writer.add_scalar(\"train/critic_loss\", critic_loss, global_step)\n",
    "\n",
    "        if iter % 1_000 == 0:\n",
    "            torch.save(policy.state_dict(), f\"{run_dir}/policy.pt\")\n",
    "\n",
    "    # Save final policy\n",
    "    torch.save(policy.state_dict(), f\"{run_dir}/policy.pt\")\n",
    "    print(f\"Saved policy to {run_dir}/policy.pt\")\n",
    "\n",
    "    # Close the environment\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "    # Average of episodic returns (for the last 5% of the training)\n",
    "    indexes = int(len(log_episodic_returns) * 0.05)\n",
    "    mean_train_return = np.mean(log_episodic_returns[-indexes:])\n",
    "    writer.add_scalar(\"rollout/mean_train_return\", mean_train_return, global_step)\n",
    "\n",
    "    return mean_train_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_render(args, run_dir):\n",
    "    # Create environment\n",
    "    env = gym.vector.SyncVectorEnv([make_env(args.env_id, capture_video=True, run_dir=run_dir)])\n",
    "\n",
    "    # Metadata about the environment\n",
    "    observation_shape = env.single_observation_space.shape\n",
    "    action_shape = env.single_action_space.shape\n",
    "    action_dim = np.prod(action_shape)\n",
    "\n",
    "    # Load policy\n",
    "    policy = ActorCriticNet(observation_shape, action_dim, args.actor_layers, args.critic_layers)\n",
    "    filename = f\"{run_dir}/policy.pt\"\n",
    "    print(f\"reading {filename}...\")\n",
    "    policy.load_state_dict(torch.load(filename))\n",
    "    policy.eval()\n",
    "\n",
    "    count_episodes = 0\n",
    "    list_rewards = []\n",
    "\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Run episodes\n",
    "    while count_episodes < 30:\n",
    "        with torch.no_grad():\n",
    "            action, _ = policy(torch.from_numpy(state).float())\n",
    "\n",
    "        action = action.cpu().numpy()\n",
    "        state, _, _, _, infos = env.step(action)\n",
    "\n",
    "        if \"final_info\" in infos:\n",
    "            info = infos[\"final_info\"][0]\n",
    "            returns = info[\"episode\"][\"r\"][0]\n",
    "            count_episodes += 1\n",
    "            list_rewards.append(returns)\n",
    "            print(f\"-> Episode {count_episodes}: {returns} returns\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return np.mean(list_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time: 2024_01_20_19_29_51\n"
     ]
    }
   ],
   "source": [
    "# Create run directory\n",
    "run_time = str(datetime.now()).replace(\" \", \"_\" ).replace(\":\", \"-\").split(\".\")[0].replace(\"-\", \"_\")\n",
    "print(f\"run time: {run_time}\")\n",
    "run_name = \"A2C_PyTorch\"\n",
    "\n",
    "run_dir = Path(f\"runs/{run_name}/{run_time}\")\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(run_dir / \"args.json\", \"w\") as fp:\n",
    "    json.dump(args.__dict__, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing training of A2C_PyTorch on HalfCheetah-v4 for 10000000 timesteps.\n",
      "Results will be saved to: runs\\A2C_PyTorch\\2024_01_20_19_29_51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([5, 16, 17]) torch.Size([5, 16, 6]) torch.Size([5, 16]) torch.Size([5, 16]) torch.Size([5, 16])\n",
      "tensor([ -8.9945,  -8.3985, -10.8777,  -9.0256, -11.5694,  -7.9853,  -8.3158,\n",
      "         -8.4930,  -8.0496, -12.1266,  -6.8231,  -7.1589,  -8.4490,  -7.9320,\n",
      "         -9.2744,  -7.3113,  -7.7720,  -7.2492,  -7.0875,  -8.6872,  -6.2833,\n",
      "         -7.5917,  -9.6206,  -7.4859, -10.9040, -11.2843, -10.3851,  -8.9012,\n",
      "         -8.6040, -12.4691,  -5.8359,  -6.8115, -10.2319,  -9.7999,  -6.6944,\n",
      "         -9.8414, -11.4924,  -9.5210, -10.3940, -10.4457,  -5.8585, -10.5886,\n",
      "        -12.0262,  -9.7057,  -8.5411,  -8.8524, -11.3480,  -7.3270,  -7.7500,\n",
      "        -10.5028,  -7.5106,  -7.2954, -11.2851,  -7.4720,  -7.4035,  -8.4039,\n",
      "        -13.9858,  -9.9141,  -7.3108,  -6.6141,  -8.8856, -12.2889, -11.7092,\n",
      "         -8.5562,  -7.4986,  -7.2988,  -9.8019,  -9.3774,  -8.8550,  -5.7936,\n",
      "         -7.4579,  -9.0327,  -7.8915,  -6.9833,  -8.8237,  -9.6240,  -7.7777,\n",
      "         -6.7863,  -8.2333,  -8.9509], grad_fn=<SumBackward1>) tensor([ 4.9083e-04, -1.1038e-04, -5.3188e-04, -1.2237e-03, -1.4055e-03,\n",
      "        -6.8297e-05,  4.7256e-04, -2.8107e-04,  1.0567e-03, -2.6875e-04,\n",
      "         5.3981e-04, -5.3912e-04,  7.2301e-04,  1.2409e-03,  8.6466e-05,\n",
      "        -3.9175e-04,  1.0088e-01,  1.2825e-01,  4.6636e-02, -5.0597e-01,\n",
      "        -5.8909e-02, -9.2176e-02, -3.4707e-01, -9.8243e-02, -6.9289e-01,\n",
      "        -4.5483e-01, -2.8200e-01, -2.9093e-01, -6.3020e-01, -7.0416e-01,\n",
      "        -1.8427e-01, -5.1813e-01, -5.8456e-02, -3.0227e-02,  4.3429e-02,\n",
      "        -8.9933e-01, -3.3960e-01, -1.2476e-01, -8.1496e-01, -2.3032e-01,\n",
      "        -2.5624e-01, -3.4422e-02, -8.2803e-01,  2.5721e-01, -4.2360e-01,\n",
      "         1.0770e-01, -7.2065e-01, -6.0547e-01, -3.2949e-02, -3.6181e-01,\n",
      "        -1.7315e-01, -3.1412e-01,  5.1648e-01,  5.5175e-02,  1.2485e-01,\n",
      "        -5.1238e-01,  9.3318e-01,  4.2486e-01, -6.2356e-01, -1.2929e-01,\n",
      "        -8.4690e-02,  1.2775e-01, -6.8980e-02, -1.8140e-01, -7.7945e-01,\n",
      "        -5.8249e-01, -4.4394e-01, -9.2712e-02,  8.0527e-01,  2.5747e-01,\n",
      "         6.6188e-01, -1.0942e-01,  3.2975e-02, -1.5697e-02, -1.1472e-01,\n",
      "         2.6303e-01, -2.7606e-01, -2.4376e-01,  2.3602e-01,  9.0366e-02],\n",
      "       grad_fn=<SqueezeBackward1>) tensor([8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136,\n",
      "        8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136,\n",
      "        8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136,\n",
      "        8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136,\n",
      "        8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136,\n",
      "        8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136,\n",
      "        8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136,\n",
      "        8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136,\n",
      "        8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136, 8.5136],\n",
      "       grad_fn=<SumBackward1>)\n",
      "tensor(13.2436, grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommencing training of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39menv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mtotal_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timesteps.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults will be saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m mean_train_return \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining - Mean returns achieved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_train_return\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 113\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, run_name, run_dir)\u001b[0m\n\u001b[0;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Log training metrics\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrollout/SPS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[43mglobal_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m), global_step)\n\u001b[0;32m    114\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, global_step)\n\u001b[0;32m    115\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/actor_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, actor_loss, global_step)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "print(f\"Commencing training of {run_name} on {args.env_id} for {args.total_timesteps} timesteps.\")\n",
    "print(f\"Results will be saved to: {run_dir}\")\n",
    "mean_train_return = train(args=args, run_name=run_name, run_dir=run_dir)\n",
    "print(f\"Training - Mean returns achieved: {mean_train_return}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rundir_to_eval = Path(f\"runs/{run_name}/2024_01_14_20_44_28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating and capturing videos on HalfCheetah-v4.\n",
      "reading runs\\A2C_PyTorch\\2024_01_14_20_44_28/policy.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\nkvch\\studia\\USD\\runs\\A2C_PyTorch\\2024_01_14_20_44_28\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Episode 1: 715.8059692382812 returns\n",
      "-> Episode 2: 1270.4517822265625 returns\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating and capturing videos on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39menv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m mean_eval_return \u001b[38;5;241m=\u001b[39m \u001b[43meval_and_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrundir_to_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation - Mean returns achieved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_eval_return\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36meval_and_render\u001b[1;34m(args, run_dir)\u001b[0m\n\u001b[0;32m     25\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m policy(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     27\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 28\u001b[0m state, _, _, _, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_info\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m infos:\n\u001b[0;32m     31\u001b[0m     info \u001b[38;5;241m=\u001b[39m infos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_info\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\vector\\vector_env.py:204\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    {}\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:149\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m [], {}\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (env, action) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions)):\n\u001b[0;32m    143\u001b[0m     (\n\u001b[0;32m    144\u001b[0m         observation,\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminateds[i],\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncateds[i],\n\u001b[0;32m    148\u001b[0m         info,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminateds[i] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncateds[i]:\n\u001b[0;32m    152\u001b[0m         old_observation, old_info \u001b[38;5;241m=\u001b[39m observation, info\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\core.py:555\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\normalize.py:143\u001b[0m, in \u001b[0;36mNormalizeReward.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, normalizing the rewards returned.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     obs, rews, terminateds, truncateds, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_vector_env:\n\u001b[0;32m    145\u001b[0m         rews \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([rews])\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\normalize.py:81\u001b[0m, in \u001b[0;36mNormalizeObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and normalizes the observation.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     obs, rews, terminateds, truncateds, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_vector_env:\n\u001b[0;32m     83\u001b[0m         obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(obs)\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\core.py:591\u001b[0m, in \u001b[0;36mActionWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the :attr:`env` :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\record_episode_statistics.py:94\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     (\n\u001b[0;32m     89\u001b[0m         observations,\n\u001b[0;32m     90\u001b[0m         rewards,\n\u001b[0;32m     91\u001b[0m         terminations,\n\u001b[0;32m     92\u001b[0m         truncations,\n\u001b[0;32m     93\u001b[0m         infos,\n\u001b[1;32m---> 94\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m     96\u001b[0m         infos, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m     97\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`info` dtype is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(infos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:183\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\monitoring\\video_recorder.py:113\u001b[0m, in \u001b[0;36mVideoRecorder.capture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcapture_frame\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Render the given `env` and add the resulting frame to the video.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame, List):\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m frame\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_env.py:409\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmujoco_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcamera_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcamera_name\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:669\u001b[0m, in \u001b[0;36mMujocoRenderer.render\u001b[1;34m(self, render_mode, camera_id, camera_name)\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m camera_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    663\u001b[0m         camera_id \u001b[38;5;241m=\u001b[39m mujoco\u001b[38;5;241m.\u001b[39mmj_name2id(\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    665\u001b[0m             mujoco\u001b[38;5;241m.\u001b[39mmjtObj\u001b[38;5;241m.\u001b[39mmjOBJ_CAMERA,\n\u001b[0;32m    666\u001b[0m             camera_name,\n\u001b[0;32m    667\u001b[0m         )\n\u001b[1;32m--> 669\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcamera_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m render_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:246\u001b[0m, in \u001b[0;36mOffScreenViewer.render\u001b[1;34m(self, render_mode, camera_id, segmentation)\u001b[0m\n\u001b[0;32m    239\u001b[0m rgb_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewport\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewport\u001b[38;5;241m.\u001b[39mheight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    242\u001b[0m depth_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewport\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewport\u001b[38;5;241m.\u001b[39mheight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    244\u001b[0m )\n\u001b[1;32m--> 246\u001b[0m \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmjr_readPixels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    249\u001b[0m     depth_img \u001b[38;5;241m=\u001b[39m depth_arr\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewport\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewport\u001b[38;5;241m.\u001b[39mwidth)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating and capturing videos on {args.env_id}.\")\n",
    "mean_eval_return = eval_and_render(args=args, run_dir=rundir_to_eval)\n",
    "print(f\"Evaluation - Mean returns achieved: {mean_eval_return}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "To embed videos, you must pass embed=True (this may make your notebook files huge)\nConsider passing Video(url='...')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Video\n\u001b[1;32m----> 3\u001b[0m \u001b[43mVideo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhalf-cheetah-video.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\IPython\\core\\display.py:1162\u001b[0m, in \u001b[0;36mVideo.__init__\u001b[1;34m(self, data, url, filename, embed, mimetype, width, height, html_attributes)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embed:\n\u001b[0;32m   1157\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\n\u001b[0;32m   1158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo embed videos, you must pass embed=True \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(this may make your notebook files huge)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider passing Video(url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m     ])\n\u001b[1;32m-> 1162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmimetype \u001b[38;5;241m=\u001b[39m mimetype\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed \u001b[38;5;241m=\u001b[39m embed\n",
      "\u001b[1;31mValueError\u001b[0m: To embed videos, you must pass embed=True (this may make your notebook files huge)\nConsider passing Video(url='...')"
     ]
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"half-cheetah-video.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
