{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.env_id = \"HalfCheetah-v4\"\n",
    "args.total_timesteps = 10_000_000\n",
    "args.num_envs = 16\n",
    "args.num_steps = 5\n",
    "args.learning_rate = 5e-4\n",
    "args.actor_layers = [64, 64]\n",
    "args.critic_layers  = [64, 64]\n",
    "args.gamma = 0.99\n",
    "args.gae = 1.0\n",
    "args.value_coef = 0.5\n",
    "args.entropy_coef = 0.01\n",
    "args.clip_grad_norm = 0.5\n",
    "args.seed = 0\n",
    "\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.num_updates = int(args.total_timesteps // args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, capture_video=False, run_dir=\".\"):\n",
    "    def thunk():\n",
    "        if capture_video:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(\n",
    "                env=env,\n",
    "                video_folder=f\"{run_dir}/videos\",\n",
    "                episode_trigger=lambda x: x,\n",
    "                disable_logger=True,\n",
    "            )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.FlattenObservation(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda state: np.clip(state, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, flags, values, last_value, args):\n",
    "    advantages = torch.zeros((args.num_steps, args.num_envs))\n",
    "    adv = torch.zeros(args.num_envs)\n",
    "\n",
    "    for i in reversed(range(args.num_steps)):\n",
    "        returns = rewards[i] + args.gamma * flags[i] * last_value\n",
    "        delta = returns - values[i]\n",
    "\n",
    "        adv = delta + args.gamma * args.gae * flags[i] * adv\n",
    "        advantages[i] = adv\n",
    "\n",
    "        last_value = values[i]\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, num_steps, num_envs, observation_shape, action_shape):\n",
    "        self.states = np.zeros((num_steps, num_envs, *observation_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((num_steps, num_envs, *action_shape), dtype=np.float32)\n",
    "        self.rewards = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        self.flags = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        self.values = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "\n",
    "        self.step = 0\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def push(self, state, action, reward, flag, value):\n",
    "        self.states[self.step] = state\n",
    "        self.actions[self.step] = action\n",
    "        self.rewards[self.step] = reward\n",
    "        self.flags[self.step] = flag\n",
    "        self.values[self.step] = value\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def get(self):\n",
    "        return (\n",
    "            torch.from_numpy(self.states),\n",
    "            torch.from_numpy(self.actions),\n",
    "            torch.from_numpy(self.rewards),\n",
    "            torch.from_numpy(self.flags),\n",
    "            torch.from_numpy(self.values),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, observation_shape, action_dim, actor_layers, critic_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor_net = self._build_net(observation_shape, actor_layers)\n",
    "        self.critic_net = self._build_net(observation_shape, critic_layers)\n",
    "\n",
    "        self.actor_net.append(self._build_linear(actor_layers[-1], action_dim, std=0.01))\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n",
    "\n",
    "        self.critic_net.append(self._build_linear(critic_layers[-1], 1, std=1.0))\n",
    "\n",
    "    def _build_linear(self, in_size, out_size, apply_init=True, std=np.sqrt(2), bias_const=0.0):\n",
    "        layer = nn.Linear(in_size, out_size)\n",
    "\n",
    "        if apply_init:\n",
    "            torch.nn.init.orthogonal_(layer.weight, std)\n",
    "            torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    def _build_net(self, observation_shape, hidden_layers):\n",
    "        layers = nn.Sequential()\n",
    "        in_size = np.prod(observation_shape)\n",
    "\n",
    "        for out_size in hidden_layers:\n",
    "            layers.append(self._build_linear(in_size, out_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            in_size = out_size\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = self.actor_net(state)\n",
    "        std = self.actor_logstd.expand_as(mean).exp()\n",
    "        distribution = Normal(mean, std)\n",
    "\n",
    "        action = distribution.sample()\n",
    "\n",
    "        value = self.critic_net(state).squeeze(-1)\n",
    "\n",
    "        return action, value\n",
    "\n",
    "    def evaluate(self, states, actions):\n",
    "        mean = self.actor_net(states)\n",
    "        std = self.actor_logstd.expand_as(mean).exp()\n",
    "        distribution = Normal(mean, std)\n",
    "\n",
    "        log_probs = distribution.log_prob(actions).sum(-1)\n",
    "        entropy = distribution.entropy().sum(-1)\n",
    "\n",
    "        values = self.critic_net(states).squeeze(-1)\n",
    "\n",
    "        return log_probs, values, entropy\n",
    "\n",
    "    def critic(self, state):\n",
    "        return self.critic_net(state).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadLogger:\n",
    "    def __init__(self, run_dir, thread_id):\n",
    "        self.run_dir = run_dir\n",
    "        self.thread_id = thread_id\n",
    "\n",
    "    def log(self, text):\n",
    "        with open(f\"{self.run_dir}/thread_{self.thread_id}.log\", \"a\") as f:\n",
    "            f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def worker(global_policy, global_optimizer, global_step, args, run_dir):\n",
    "    logger = ThreadLogger(run_dir, threading.get_ident())\n",
    "\n",
    "    try:\n",
    "        logger.log(f\"Worker {threading.get_ident()} started\")\n",
    "        env = make_env(args.env_id, run_dir=run_dir)()\n",
    "\n",
    "        local_policy = ActorCriticNet(\n",
    "            env.observation_space.shape,\n",
    "            env.action_space.shape[0],\n",
    "            args.actor_layers,\n",
    "            args.critic_layers,\n",
    "        )\n",
    "\n",
    "        rollout_buffer = RolloutBuffer(\n",
    "            args.num_steps,\n",
    "            args.num_envs,\n",
    "            env.observation_space.shape,\n",
    "            env.action_space.shape,\n",
    "        )\n",
    "\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while global_step.value < args.total_timesteps:\n",
    "            local_policy.load_state_dict(global_policy.state_dict())\n",
    "\n",
    "            for _ in range(args.num_steps):\n",
    "                global_step.value += 1\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action, value = local_policy(torch.from_numpy(state).float())\n",
    "\n",
    "                action = action.detach().numpy()\n",
    "                next_state, reward, terminated, truncated, infos = env.step(action)\n",
    "\n",
    "                flag = 1.0 - np.logical_or(terminated, truncated)\n",
    "                value = value.detach().numpy()\n",
    "\n",
    "                rollout_buffer.push(state, action, reward, flag, value)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            states, actions, rewards, flags, values = rollout_buffer.get()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                last_value = local_policy.critic(torch.from_numpy(state).float()).detach().numpy()\n",
    "\n",
    "            advantages = compute_advantages(rewards, flags, values, last_value, args)\n",
    "            td_target = advantages + values\n",
    "\n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            log_probs, td_predict, entropy = local_policy.evaluate(states, actions)\n",
    "\n",
    "            actor_loss = -(log_probs * advantages).mean()\n",
    "            critic_loss = mse_loss(td_target, td_predict)\n",
    "            entropy_loss = entropy.mean()\n",
    "\n",
    "            loss = actor_loss + args.value_coef * critic_loss - args.entropy_coef * entropy_loss\n",
    "\n",
    "            local_policy.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            clip_grad_norm_(local_policy.parameters(), args.clip_grad_norm)\n",
    "\n",
    "            with global_optimizer.get_lock():\n",
    "                for local_param, global_param in zip(local_policy.parameters(), global_policy.parameters()):\n",
    "                    global_param._grad = local_param.grad\n",
    "\n",
    "                global_optimizer.step()\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "class Worker(mp.Process):\n",
    "    def __init__(self, global_policy, global_optimizer, global_step, args, run_dir):\n",
    "        super().__init__()\n",
    "        self.global_policy = global_policy\n",
    "        self.global_optimizer = global_optimizer\n",
    "        self.global_step = global_step\n",
    "        self.args = args\n",
    "        self.run_dir = run_dir\n",
    "\n",
    "    def run(self):\n",
    "        worker(self.global_policy, self.global_optimizer, self.global_step, self.args, self.run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from multiprocessing import Lock\n",
    "\n",
    "class GlobalOptimizer:\n",
    "    def __init__(self, global_policy, args):\n",
    "        self.global_policy = global_policy\n",
    "        self.optimizer = optim.RMSprop(global_policy.parameters(), lr=args.learning_rate, alpha=0.99, eps=1e-5)\n",
    "\n",
    "        self.lock = Lock()\n",
    "\n",
    "    def step(self):\n",
    "        with self.lock:\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def get_lock(self):\n",
    "        return self.lock\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    run_dir = Path(f\"runs/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "    run_dir.mkdir(parents=True)\n",
    "\n",
    "    with open(run_dir / \"args.json\", \"w\") as f:\n",
    "        json.dump(vars(args), f, indent=4)\n",
    "\n",
    "    env = make_env(args.env_id, capture_video=True, run_dir=run_dir)()\n",
    "\n",
    "    global_policy = ActorCriticNet(\n",
    "        env.observation_space.shape,\n",
    "        env.action_space.shape[0],\n",
    "        args.actor_layers,\n",
    "        args.critic_layers,\n",
    "    )\n",
    "\n",
    "    global_optimizer = GlobalOptimizer(global_policy, args)\n",
    "\n",
    "    global_step = mp.Value('i', 0)\n",
    "\n",
    "    workers = [Worker(global_policy, global_optimizer, global_step, args, run_dir) for _ in range(args.num_envs)]\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.start()\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
