{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, env.action_space.n),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(nn.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = torch.tensor(env.reset()[0], dtype=torch.float)\n",
    "done = False\n",
    "Actions, States, Rewards = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_state\n",
    "\n",
    "while not done:\n",
    "    probs = nn(state)\n",
    "    dist = torch.distributions.Categorical(probs=probs)\n",
    "    action = dist.sample().item()\n",
    "    state, r, done, _, _ = env.step(action)\n",
    "\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "    Actions.append(torch.tensor(action, dtype=torch.int))\n",
    "    States.append(state)\n",
    "    Rewards.append(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0474,  0.1537, -0.0310, -0.3426]),\n",
       " tensor([ 0.0504,  0.3492, -0.0378, -0.6449]),\n",
       " tensor([ 0.0574,  0.1547, -0.0507, -0.3644]),\n",
       " tensor([ 0.0605, -0.0397, -0.0580, -0.0882]),\n",
       " tensor([ 0.0597,  0.1562, -0.0598, -0.3986]),\n",
       " tensor([ 0.0628, -0.0380, -0.0678, -0.1253]),\n",
       " tensor([ 0.0621, -0.2321, -0.0703,  0.1452]),\n",
       " tensor([ 0.0574, -0.0361, -0.0674, -0.1688]),\n",
       " tensor([ 0.0567,  0.1600, -0.0707, -0.4819]),\n",
       " tensor([ 0.0599, -0.0341, -0.0804, -0.2123]),\n",
       " tensor([ 0.0592, -0.2280, -0.0846,  0.0539]),\n",
       " tensor([ 0.0547, -0.0318, -0.0836, -0.2642]),\n",
       " tensor([ 0.0540, -0.2256, -0.0888,  0.0010]),\n",
       " tensor([ 0.0495, -0.4193, -0.0888,  0.2644]),\n",
       " tensor([ 0.0411, -0.2231, -0.0835, -0.0549]),\n",
       " tensor([ 0.0367, -0.4169, -0.0846,  0.2103]),\n",
       " tensor([ 0.0283, -0.2207, -0.0804, -0.1079]),\n",
       " tensor([ 0.0239, -0.0245, -0.0826, -0.4248]),\n",
       " tensor([ 0.0234, -0.2184, -0.0911, -0.1592]),\n",
       " tensor([ 0.0191, -0.0221, -0.0943, -0.4792]),\n",
       " tensor([ 0.0186,  0.1743, -0.1038, -0.8001]),\n",
       " tensor([ 0.0221, -0.0193, -0.1198, -0.5418]),\n",
       " tensor([ 0.0217,  0.1773, -0.1307, -0.8697]),\n",
       " tensor([ 0.0253,  0.3739, -0.1481, -1.2004]),\n",
       " tensor([ 0.0327,  0.1810, -0.1721, -0.9576]),\n",
       " tensor([ 0.0364,  0.3780, -0.1912, -1.2990]),\n",
       " tensor([ 0.0439,  0.1857, -0.2172, -1.0718])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DiscountedRewards = []\n",
    "\n",
    "for t in range(len(Rewards)):\n",
    "    G = 0.0\n",
    "    for k, r in enumerate(Rewards[t:]):\n",
    "        G += (gamma**k)*k\n",
    "    DiscountedRewards.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.625232995919472,\n",
       " 19.54665281764452,\n",
       " 19.45220548798713,\n",
       " 19.338868692398258,\n",
       " 19.203100656015756,\n",
       " 19.04076930816711,\n",
       " 18.84707849539316,\n",
       " 18.61649419447179,\n",
       " 18.342675337127666,\n",
       " 18.01841616395699,\n",
       " 17.635610195630495,\n",
       " 17.185250232893445,\n",
       " 16.657484651560964,\n",
       " 16.041758140006404,\n",
       " 15.327075581952005,\n",
       " 14.502441861120005,\n",
       " 13.557549056000004,\n",
       " 12.483807232000004,\n",
       " 11.275847680000004,\n",
       " 9.933670400000002,\n",
       " 8.465664000000002,\n",
       " 6.892800000000001,\n",
       " 5.2544,\n",
       " 3.6160000000000005,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DiscountedRewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay experience and learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, action, G in zip(States, Actions, DiscountedRewards):\n",
    "    probs = nn(state)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    loss = - log_prob*G\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1.0\n",
      "Reward: 1.0\n",
      "Reward: 1.0\n",
      "Reward: 1.0\n",
      "Reward: 1.0\n",
      "Reward: 1.0\n",
      "Reward: 1.0\n",
      "Reward: 1.0\n",
      "Reward: 1.0\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "state = torch.tensor(env.reset()[0], dtype=torch.float)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    probs = nn(state)\n",
    "    dist = torch.distributions.Categorical(probs=probs)\n",
    "    action = dist.sample().item()\n",
    "    state, r, done, _, _ = env.step(action)\n",
    "\n",
    "    print(f'Reward: {r}')\n",
    "\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "lr = 0.01\n",
    "episodes = 2000\n",
    "hid_layer = 128\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, hid_layer),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hid_layer, env.action_space.n),\n",
    "    torch.nn.Softmax()\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=lr, betas=[beta1, beta2])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=100, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_and_learn_from_it():\n",
    "    done = False\n",
    "    Actions, States, Rewards = [], [], []\n",
    "    state = torch.tensor(env.reset()[0], dtype=torch.float).to(device)\n",
    "\n",
    "    while not done:\n",
    "        probs = model(state).cpu()\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample().item()\n",
    "        \n",
    "        state, r, done, _, _ = env.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float).to(device)\n",
    "\n",
    "        Actions.append(torch.tensor(action, dtype=torch.int).to(device))\n",
    "        States.append(state)\n",
    "        Rewards.append(r)\n",
    "\n",
    "    print(f'Rewards: {len(Rewards)}')\n",
    "\n",
    "    # now when done, let's calc discounted rewards in each step\n",
    "    DiscountedRewards = []\n",
    "\n",
    "    for t in range(len(Rewards)):\n",
    "        G = 0.0\n",
    "        for k, r in enumerate(Rewards[t:]):\n",
    "            G += (gamma**k)*r\n",
    "        DiscountedRewards.append(G)\n",
    "    \n",
    "    DiscountedRewards = torch.tensor(DiscountedRewards)\n",
    "    DiscountedRewards = (DiscountedRewards - DiscountedRewards.mean()) / (DiscountedRewards.std() + 1e-9)  # Normalize\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # now when have all data, let's learn\n",
    "    for state, action, G in zip(States, Actions, DiscountedRewards):\n",
    "        probs = model(state).to(device)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        loss += - log_prob*G\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optim.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    return len(Rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkvch\\studia\\USD\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: 35\n",
      "Rewards: 15\n",
      "Rewards: 16\n",
      "Rewards: 27\n",
      "Rewards: 31\n",
      "Rewards: 23\n",
      "Rewards: 24\n",
      "Rewards: 26\n",
      "Rewards: 44\n",
      "Rewards: 55\n",
      "Rewards: 40\n",
      "Rewards: 20\n",
      "Rewards: 45\n",
      "Rewards: 19\n",
      "Rewards: 57\n",
      "Rewards: 23\n",
      "Rewards: 22\n",
      "Rewards: 21\n",
      "Rewards: 16\n",
      "Rewards: 23\n",
      "Rewards: 26\n",
      "Rewards: 12\n",
      "Rewards: 28\n",
      "Rewards: 50\n",
      "Rewards: 20\n",
      "Rewards: 31\n",
      "Rewards: 23\n",
      "Rewards: 102\n",
      "Rewards: 20\n",
      "Rewards: 17\n",
      "Rewards: 132\n",
      "Rewards: 75\n",
      "Rewards: 38\n",
      "Rewards: 29\n",
      "Rewards: 166\n",
      "Rewards: 12\n",
      "Rewards: 126\n",
      "Rewards: 50\n",
      "Rewards: 174\n",
      "Rewards: 35\n",
      "Rewards: 64\n",
      "Rewards: 38\n",
      "Rewards: 50\n",
      "Rewards: 65\n",
      "Rewards: 27\n",
      "Rewards: 19\n",
      "Rewards: 40\n",
      "Rewards: 175\n",
      "Rewards: 20\n",
      "Rewards: 20\n",
      "Rewards: 27\n",
      "Rewards: 15\n",
      "Rewards: 20\n",
      "Rewards: 15\n",
      "Rewards: 20\n",
      "Rewards: 17\n",
      "Rewards: 23\n",
      "Rewards: 33\n",
      "Rewards: 18\n",
      "Rewards: 38\n",
      "Rewards: 113\n",
      "Rewards: 155\n",
      "Rewards: 189\n",
      "Rewards: 66\n",
      "Rewards: 345\n",
      "Rewards: 82\n",
      "Rewards: 387\n",
      "Learned (probably).\n"
     ]
    }
   ],
   "source": [
    "lifetimes = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    lifetimes.append(run_episode_and_learn_from_it())\n",
    "\n",
    "    if np.mean(lifetimes[-5:]) > 200:\n",
    "        break\n",
    "\n",
    "print('Learned (probably).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(model.state_dict(), 'weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "model.load_state_dict(torch.load('weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_and_show():\n",
    "    done = False\n",
    "    state = torch.tensor(env.reset()[0], dtype=torch.float).to(device)\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        probs = model(state).cpu()\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample().item()\n",
    "\n",
    "        state, r, done, _, _ = env.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    run_episode_and_show()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
