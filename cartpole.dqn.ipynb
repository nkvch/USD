{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_number = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "lr = 5e-5\n",
    "hid_layer1 = 64\n",
    "hid_layer2 = 128\n",
    "batch_size = 32\n",
    "q_target_update_frequency = 1000\n",
    "\n",
    "epsiolon_start = 1.0\n",
    "epsilon_end = 0.02\n",
    "epsilon_decay = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hid_layer1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hid_layer1, hid_layer2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hid_layer2, action_number)\n",
    ").to(device)\n",
    "\n",
    "Q_target = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hid_layer1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hid_layer1, hid_layer2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hid_layer2, action_number)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(Q.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity, min_replay_size):\n",
    "#         self.capacity = capacity\n",
    "#         self.min_replay_size = min_replay_size\n",
    "#         self.buffer = [None for i in range(capacity)]\n",
    "#         self.position = 0\n",
    "\n",
    "#     def push(self, state, action, reward, next_state, done):\n",
    "#         self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         # filter out the None\n",
    "#         return random.sample([i for i in self.buffer if i is not None], batch_size)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.buffer)\n",
    "    \n",
    "#     def is_ready(self):\n",
    "#         return len([i for i in self.buffer if i is not None]) >= self.min_replay_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, min_replay_size):\n",
    "        self.capacity = capacity\n",
    "        self.min_replay_size = min_replay_size\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def is_ready(self):\n",
    "        return len(self.buffer) >= self.min_replay_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(50000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, randomness):\n",
    "    risk = random.random() <= randomness\n",
    "\n",
    "    q = Q(torch.tensor(state, dtype=torch.float).to(device)).to(device)\n",
    "\n",
    "    action = random.choice([possible for possible in range(action_number)]) if risk \\\n",
    "        else torch.argmax(q).item()\n",
    "\n",
    "    new_state, r, done, _, _ = env.step(action)\n",
    "\n",
    "    replay_buffer.push(state, action, r, new_state, done)\n",
    "\n",
    "    if not replay_buffer.is_ready():\n",
    "        return done, None\n",
    "\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "    states = torch.tensor([i[0] for i in batch], dtype=torch.float).to(device)\n",
    "    actions = torch.tensor([i[1] for i in batch], dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor([i[2] for i in batch], dtype=torch.float).to(device)\n",
    "    new_states = torch.tensor([i[3] for i in batch], dtype=torch.float).to(device)\n",
    "    dones = torch.tensor([i[4] for i in batch], dtype=torch.int).to(device)\n",
    "\n",
    "    q = Q(states).to(device)\n",
    "    q_target = Q_target(new_states).to(device)\n",
    "\n",
    "    y = rewards + (1 - dones)*(gamma*torch.max(q_target, dim=1, keepdim=True)[0])\n",
    "\n",
    "    q_actions = torch.gather(input=q, dim=1, index=actions.unsqueeze(-1))\n",
    "\n",
    "    loss = torch.nn.functional.smooth_l1_loss(q_actions, y)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # scheduler.step(loss)\n",
    "\n",
    "    return done, loss.item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_population():\n",
    "    state = env.reset()[0]\n",
    "    while not replay_buffer.is_ready():\n",
    "        done, _ = step(state, 1.0)\n",
    "        if done:\n",
    "            state = env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(curr_steps):\n",
    "    episode_steps = 0\n",
    "    episode_avg_loss = 0\n",
    "\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        step_randomness = np.interp(curr_steps + episode_steps, [0, epsilon_decay], [epsiolon_start, epsilon_end])\n",
    "        done, loss = step(state, step_randomness)\n",
    "        episode_steps += 1\n",
    "\n",
    "        if (curr_steps + episode_steps) % q_target_update_frequency == 0:\n",
    "            Q_target.load_state_dict(Q.state_dict())\n",
    "\n",
    "    episode_avg_loss = (episode_avg_loss * episode_steps + loss) / (episode_steps + 1)\n",
    "\n",
    "    return episode_steps, episode_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkvch\\AppData\\Local\\Temp\\ipykernel_15680\\3932050584.py:31: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.smooth_l1_loss(q_actions, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0001. Loss: 0.49142. Score: 021. Average score: 10.50.\n",
      "Episode: 0002. Loss: 0.23090. Score: 052. Average score: 24.33.\n",
      "Episode: 0003. Loss: 0.76201. Score: 013. Average score: 21.50.\n",
      "Episode: 0004. Loss: 0.23780. Score: 036. Average score: 24.40.\n",
      "Episode: 0005. Loss: 0.35064. Score: 015. Average score: 22.83.\n",
      "Episode: 0006. Loss: 0.36259. Score: 012. Average score: 21.29.\n",
      "Episode: 0007. Loss: 0.71516. Score: 011. Average score: 20.00.\n",
      "Episode: 0008. Loss: 0.25598. Score: 015. Average score: 19.44.\n",
      "Episode: 0009. Loss: 0.48041. Score: 027. Average score: 20.20.\n",
      "Episode: 0010. Loss: 0.34748. Score: 020. Average score: 20.18.\n",
      "Episode: 0011. Loss: 0.39721. Score: 012. Average score: 19.50.\n",
      "Episode: 0012. Loss: 0.46540. Score: 016. Average score: 19.23.\n",
      "Episode: 0013. Loss: 0.45667. Score: 014. Average score: 18.86.\n",
      "Episode: 0014. Loss: 0.60214. Score: 012. Average score: 18.40.\n",
      "Episode: 0015. Loss: 0.26739. Score: 032. Average score: 19.25.\n",
      "Episode: 0016. Loss: 0.06499. Score: 055. Average score: 21.35.\n",
      "Episode: 0017. Loss: 0.24746. Score: 026. Average score: 21.61.\n",
      "Episode: 0018. Loss: 0.22470. Score: 016. Average score: 21.32.\n",
      "Episode: 0019. Loss: 0.31761. Score: 018. Average score: 21.15.\n",
      "Episode: 0020. Loss: 0.19552. Score: 015. Average score: 20.86.\n",
      "Episode: 0021. Loss: 0.36026. Score: 024. Average score: 21.00.\n",
      "Episode: 0022. Loss: 0.24695. Score: 035. Average score: 21.61.\n",
      "Episode: 0023. Loss: 0.36553. Score: 014. Average score: 21.29.\n",
      "Episode: 0024. Loss: 0.31243. Score: 032. Average score: 21.72.\n",
      "Episode: 0025. Loss: 0.54208. Score: 014. Average score: 21.42.\n",
      "Episode: 0026. Loss: 0.42301. Score: 013. Average score: 21.11.\n",
      "Episode: 0027. Loss: 0.40191. Score: 013. Average score: 20.82.\n",
      "Episode: 0028. Loss: 0.18536. Score: 045. Average score: 21.66.\n",
      "Episode: 0029. Loss: 0.54050. Score: 011. Average score: 21.30.\n",
      "Episode: 0030. Loss: 0.39465. Score: 016. Average score: 21.13.\n",
      "Episode: 0031. Loss: 0.45023. Score: 023. Average score: 21.19.\n",
      "Episode: 0032. Loss: 0.45592. Score: 020. Average score: 21.15.\n",
      "Episode: 0033. Loss: 0.14568. Score: 033. Average score: 21.50.\n",
      "Episode: 0034. Loss: 0.42027. Score: 023. Average score: 21.54.\n",
      "Episode: 0035. Loss: 0.43436. Score: 012. Average score: 21.28.\n",
      "Episode: 0036. Loss: 0.41139. Score: 013. Average score: 21.05.\n",
      "Episode: 0037. Loss: 0.49320. Score: 021. Average score: 21.05.\n",
      "Episode: 0038. Loss: 0.93450. Score: 010. Average score: 20.77.\n",
      "Episode: 0039. Loss: 0.25004. Score: 033. Average score: 21.07.\n",
      "Episode: 0040. Loss: 0.93500. Score: 009. Average score: 20.78.\n",
      "Episode: 0041. Loss: 0.42617. Score: 016. Average score: 20.67.\n",
      "Episode: 0042. Loss: 0.39131. Score: 017. Average score: 20.58.\n",
      "Episode: 0043. Loss: 0.58935. Score: 010. Average score: 20.34.\n",
      "Episode: 0044. Loss: 0.21687. Score: 039. Average score: 20.76.\n",
      "Episode: 0045. Loss: 0.30629. Score: 028. Average score: 20.91.\n",
      "Episode: 0046. Loss: 0.14736. Score: 020. Average score: 20.89.\n",
      "Episode: 0047. Loss: 0.21454. Score: 015. Average score: 20.77.\n",
      "Episode: 0048. Loss: 0.17196. Score: 018. Average score: 20.71.\n",
      "Episode: 0049. Loss: 0.21630. Score: 023. Average score: 20.76.\n",
      "Episode: 0050. Loss: 0.20848. Score: 030. Average score: 20.94.\n",
      "Episode: 0051. Loss: 0.40468. Score: 032. Average score: 21.15.\n",
      "Episode: 0052. Loss: 0.55371. Score: 018. Average score: 21.09.\n",
      "Episode: 0053. Loss: 0.67071. Score: 012. Average score: 20.93.\n",
      "Episode: 0054. Loss: 0.77528. Score: 010. Average score: 20.73.\n",
      "Episode: 0055. Loss: 0.96194. Score: 009. Average score: 20.52.\n",
      "Episode: 0056. Loss: 0.21218. Score: 024. Average score: 20.58.\n",
      "Episode: 0057. Loss: 0.22396. Score: 024. Average score: 20.64.\n",
      "Episode: 0058. Loss: 0.73246. Score: 016. Average score: 20.56.\n",
      "Episode: 0059. Loss: 0.15648. Score: 032. Average score: 20.75.\n",
      "Episode: 0060. Loss: 0.57423. Score: 016. Average score: 20.67.\n",
      "Episode: 0061. Loss: 0.20389. Score: 022. Average score: 20.69.\n",
      "Episode: 0062. Loss: 0.99389. Score: 012. Average score: 20.56.\n",
      "Episode: 0063. Loss: 0.06959. Score: 035. Average score: 20.78.\n",
      "Episode: 0064. Loss: 0.70449. Score: 017. Average score: 20.72.\n",
      "Episode: 0065. Loss: 0.63450. Score: 015. Average score: 20.64.\n",
      "Episode: 0066. Loss: 0.21221. Score: 031. Average score: 20.79.\n",
      "Episode: 0067. Loss: 0.31600. Score: 035. Average score: 21.00.\n",
      "Episode: 0068. Loss: 0.34244. Score: 013. Average score: 20.88.\n",
      "Episode: 0069. Loss: 0.53455. Score: 011. Average score: 20.74.\n",
      "Episode: 0070. Loss: 0.70006. Score: 014. Average score: 20.65.\n",
      "Episode: 0071. Loss: 0.20610. Score: 029. Average score: 20.76.\n",
      "Episode: 0072. Loss: 1.69545. Score: 008. Average score: 20.59.\n",
      "Episode: 0073. Loss: 0.26953. Score: 034. Average score: 20.77.\n",
      "Episode: 0074. Loss: 0.06099. Score: 074. Average score: 21.48.\n",
      "Episode: 0075. Loss: 0.34927. Score: 012. Average score: 21.36.\n",
      "Episode: 0076. Loss: 0.12351. Score: 017. Average score: 21.30.\n",
      "Episode: 0077. Loss: 0.56418. Score: 013. Average score: 21.19.\n",
      "Episode: 0078. Loss: 0.42028. Score: 014. Average score: 21.10.\n",
      "Episode: 0079. Loss: 0.33433. Score: 023. Average score: 21.12.\n",
      "Episode: 0080. Loss: 0.49163. Score: 015. Average score: 21.05.\n",
      "Episode: 0081. Loss: 0.15886. Score: 023. Average score: 21.07.\n",
      "Episode: 0082. Loss: 0.55995. Score: 012. Average score: 20.96.\n",
      "Episode: 0083. Loss: 0.11028. Score: 021. Average score: 20.96.\n",
      "Episode: 0084. Loss: 1.03674. Score: 011. Average score: 20.85.\n",
      "Episode: 0085. Loss: 0.85045. Score: 014. Average score: 20.77.\n",
      "Episode: 0086. Loss: 0.67755. Score: 011. Average score: 20.66.\n",
      "Episode: 0087. Loss: 0.27640. Score: 020. Average score: 20.65.\n",
      "Episode: 0088. Loss: 0.12292. Score: 047. Average score: 20.94.\n",
      "Episode: 0089. Loss: 0.26571. Score: 023. Average score: 20.97.\n",
      "Episode: 0090. Loss: 0.25866. Score: 025. Average score: 21.01.\n",
      "Episode: 0091. Loss: 0.69126. Score: 009. Average score: 20.88.\n",
      "Episode: 0092. Loss: 0.18118. Score: 056. Average score: 21.26.\n",
      "Episode: 0093. Loss: 0.68020. Score: 013. Average score: 21.17.\n",
      "Episode: 0094. Loss: 0.38991. Score: 029. Average score: 21.25.\n",
      "Episode: 0095. Loss: 0.93938. Score: 010. Average score: 21.14.\n",
      "Episode: 0096. Loss: 0.81768. Score: 011. Average score: 21.03.\n",
      "Episode: 0097. Loss: 0.24489. Score: 019. Average score: 21.01.\n",
      "Episode: 0098. Loss: 0.11233. Score: 022. Average score: 21.02.\n",
      "Episode: 0099. Loss: 0.53349. Score: 012. Average score: 20.93.\n",
      "Episode: 0100. Loss: 0.30445. Score: 042. Average score: 21.14.\n",
      "Episode: 0101. Loss: 0.70127. Score: 012. Average score: 21.05.\n",
      "Episode: 0102. Loss: 0.16407. Score: 030. Average score: 21.14.\n",
      "Episode: 0103. Loss: 0.32701. Score: 023. Average score: 21.15.\n",
      "Episode: 0104. Loss: 0.54320. Score: 013. Average score: 21.08.\n",
      "Episode: 0105. Loss: 0.21974. Score: 015. Average score: 21.02.\n",
      "Episode: 0106. Loss: 0.70022. Score: 015. Average score: 20.96.\n",
      "Episode: 0107. Loss: 0.41602. Score: 026. Average score: 21.01.\n",
      "Episode: 0108. Loss: 0.34346. Score: 014. Average score: 20.94.\n",
      "Episode: 0109. Loss: 0.86306. Score: 012. Average score: 20.86.\n",
      "Episode: 0110. Loss: 0.30962. Score: 011. Average score: 20.77.\n",
      "Episode: 0111. Loss: 0.96532. Score: 009. Average score: 20.67.\n",
      "Episode: 0112. Loss: 0.19014. Score: 046. Average score: 20.89.\n",
      "Episode: 0113. Loss: 0.14442. Score: 058. Average score: 21.22.\n",
      "Episode: 0114. Loss: 0.41872. Score: 022. Average score: 21.23.\n",
      "Episode: 0115. Loss: 0.47419. Score: 022. Average score: 21.23.\n",
      "Episode: 0116. Loss: 0.61323. Score: 015. Average score: 21.18.\n",
      "Episode: 0117. Loss: 0.17768. Score: 021. Average score: 21.18.\n",
      "Episode: 0118. Loss: 0.70167. Score: 012. Average score: 21.10.\n",
      "Episode: 0119. Loss: 0.51201. Score: 011. Average score: 21.02.\n",
      "Episode: 0120. Loss: 0.24235. Score: 016. Average score: 20.98.\n",
      "Episode: 0121. Loss: 0.44368. Score: 021. Average score: 20.98.\n",
      "Episode: 0122. Loss: 0.47576. Score: 022. Average score: 20.98.\n",
      "Episode: 0123. Loss: 0.66332. Score: 017. Average score: 20.95.\n",
      "Episode: 0124. Loss: 0.58374. Score: 016. Average score: 20.91.\n",
      "Episode: 0125. Loss: 0.63190. Score: 015. Average score: 20.87.\n",
      "Episode: 0126. Loss: 0.40251. Score: 017. Average score: 20.83.\n",
      "Episode: 0127. Loss: 0.88084. Score: 008. Average score: 20.73.\n",
      "Episode: 0128. Loss: 0.11562. Score: 036. Average score: 20.85.\n",
      "Episode: 0129. Loss: 0.33035. Score: 010. Average score: 20.77.\n",
      "Episode: 0130. Loss: 0.47792. Score: 016. Average score: 20.73.\n",
      "Episode: 0131. Loss: 0.14946. Score: 028. Average score: 20.79.\n",
      "Episode: 0132. Loss: 0.18744. Score: 045. Average score: 20.97.\n",
      "Episode: 0133. Loss: 0.37002. Score: 013. Average score: 20.91.\n",
      "Episode: 0134. Loss: 0.27297. Score: 011. Average score: 20.84.\n",
      "Episode: 0135. Loss: 0.50995. Score: 013. Average score: 20.78.\n",
      "Episode: 0136. Loss: 0.41340. Score: 022. Average score: 20.79.\n",
      "Episode: 0137. Loss: 0.64174. Score: 019. Average score: 20.78.\n",
      "Episode: 0138. Loss: 0.22341. Score: 026. Average score: 20.81.\n",
      "Episode: 0139. Loss: 0.50715. Score: 011. Average score: 20.74.\n",
      "Episode: 0140. Loss: 0.76165. Score: 016. Average score: 20.71.\n",
      "Episode: 0141. Loss: 0.28589. Score: 014. Average score: 20.66.\n",
      "Saved model.\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "steps = 0\n",
    "episodes = 0\n",
    "avg_score = 0\n",
    "\n",
    "# run the agent, and save the model on process exit\n",
    "try:\n",
    "    init_population()\n",
    "    while True:\n",
    "        episode_steps, episode_avg_loss = run_episode(steps)\n",
    "\n",
    "        steps += episode_steps\n",
    "        episodes += 1\n",
    "        avg_score = (avg_score * episodes + episode_steps) / (episodes + 1)\n",
    "\n",
    "        print(f'Episode: {episodes:04d}. Loss: {episode_avg_loss:.5f}. Score: {episode_steps:03d}. Average score: {avg_score:.2f}.')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    torch.save(Q.state_dict(), 'cartpole.dqn.pth')\n",
    "    print('Saved model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
